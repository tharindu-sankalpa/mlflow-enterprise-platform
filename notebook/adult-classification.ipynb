{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe41d4c",
   "metadata": {},
   "source": [
    "## Connect to MLflow in Your Python Code\n",
    "\n",
    "Now, in your Jupyter notebook or Python script, you can load these environment variables and tell MLflow where to send its data.\n",
    "\n",
    "\n",
    "How to use mlflow.start_run() and mlflow.end_run() in a Notebook:\n",
    "\n",
    "\n",
    "In a notebook environment, you often want a single MLflow run to span across multiple cells. For example, one cell might load data, another might preprocess it, and a third might train a model‚Ää-‚Ääall part of the same experimental \"run.\"\n",
    "Here's how you can set it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ff4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run Name: Initial_Setup_and_Connection_Test\n",
      "MLflow Run ID: 0294f20a6c274613aeaa8a140c210d77\n",
      "MLflow Experiment ID: 2\n",
      "MLflow tracking URI: http://135.235.251.124\n",
      "MLflow artifact URI: wasbs://artifactroot@tharindumlflow0aa3981a.blob.core.windows.net/2/0294f20a6c274613aeaa8a140c210d77/artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'successful'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 1: Environment Setup and Start MLflow Run ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials and URI from environment variables\n",
    "MLFLOW_USERNAME = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "MLFLOW_PASSWORD = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "# These lines are crucial for MLflow to authenticate with your server\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_USERNAME\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_PASSWORD\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Define an experiment name\n",
    "# If the experiment doesn't exist, MLflow creates it.\n",
    "EXPERIMENT_NAME = \"UCI Adult Income Prediction - Centralized\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Start a new run manually. We'll end it later.\n",
    "# It's good practice to give your run a descriptive name.\n",
    "current_run = mlflow.start_run(run_name=\"Initial_Setup_and_Connection_Test\")\n",
    "print(f\"MLflow Run Name: {current_run.data.tags.get('mlflow.runName')}\")\n",
    "print(f\"MLflow Run ID: {current_run.info.run_id}\")\n",
    "print(f\"MLflow Experiment ID: {current_run.info.experiment_id}\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow artifact URI: {mlflow.get_artifact_uri()}\")\n",
    "\n",
    "# You can log a simple parameter to test\n",
    "mlflow.log_param(\"connection_status\", \"successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95c2ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Initial_Setup_and_Connection_Test at: http://135.235.251.124/#/experiments/2/runs/0294f20a6c274613aeaa8a140c210d77\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "MLflow run ended.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell X: End the MLflow Run ---\n",
    "if mlflow.active_run(): # Check if a run is active before trying to end it\n",
    "    mlflow.end_run()\n",
    "    print(\"MLflow run ended.\")\n",
    "else:\n",
    "    print(\"No active MLflow run to end.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ff605",
   "metadata": {},
   "source": [
    "# Practical Application: The UCI Adult Income¬†Dataset\n",
    "With our MLflow environment connected to our central server, let's get practical. We'll use the well-known UCI Adult Income dataset. The goal is to predict whether an individual's income is more than $50,000 per year based on census data. This is a binary classification problem (two possible outcomes).\n",
    "Throughout this project, we'll see how to use MLflow to keep track of our data cleaning, exploratory data analysis (EDA), model training, and model checking steps.\n",
    "\n",
    "# Phase 1: Data Ingestion and Initial Preparation\n",
    "First, we need to get the dataset. The ucimlrepo library is a handy way to download datasets directly from the UCI Machine Learning Repository.\n",
    "\n",
    "\n",
    "- [IMPORTANT NOTE FOR YOU]: The following Python code block is designed to be run in a Jupyter notebook cell. It assumes you've already run the setup cell (Cell 1 from \"Connect to MLflow in Your Python Code\") that sets the MLflow tracking URI and starts an MLflow run. We will be logging to the active run started previously, or if you prefer, you can start a new, dedicated run for this phase.\n",
    "\n",
    "Let's create a new run specifically for data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd948a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run EDA at: http://135.235.251.124/#/experiments/2/runs/a5e9075e3c794825a523a791000dcd19\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "Ended previous active run.\n",
      "Starting new MLflow Run for Data Ingestion: Data_Ingestion_and_Initial_Prep\n",
      "Run ID: ec782e2e90684de5b2811a3cbc3a6269\n",
      "Fetching UCI Adult dataset...\n",
      "Dataset fetched successfully.\n",
      "\n",
      "Combined DataFrame head (first 5 rows):\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n",
      "\n",
      "Converted integer columns to float: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "\n",
      "Logging dataset as MLflow input. Target column: 'income'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'https://archive.ics.uci.edu/static/public/2/data.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset logged as MLflow input.\n",
      "\n",
      "UCI dataset metadata logged to MLflow artifacts: temp_data_ingestion_artifacts/uci_adult_dataset_metadata.json\n",
      "UCI dataset variable info logged to MLflow artifacts: temp_data_ingestion_artifacts/uci_adult_dataset_variables.json\n",
      "Unique values for categorical features logged to MLflow artifacts: temp_data_ingestion_artifacts/categorical_features_unique_values.json\n",
      "\n",
      "Logging dataset overview parameters to MLflow...\n",
      "Dataset overview parameters logged.\n",
      "Logged a sample of raw data to MLflow artifacts: temp_data_ingestion_artifacts/raw_data_sample.csv\n",
      "\n",
      "Data ingestion and initial preparation run finished. Check MLflow UI for run ID: ec782e2e90684de5b2811a3cbc3a6269\n",
      "üèÉ View run Data_Ingestion_and_Initial_Prep at: http://135.235.251.124/#/experiments/4/runs/ec782e2e90684de5b2811a3cbc3a6269\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/4\n",
      "MLflow run for Data Ingestion ended.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Data Ingestion and Initial Preparation ---\n",
    "# Make sure you've run the initial MLflow setup cell first to set the tracking URI and experiment.\n",
    "\n",
    "import pandas as pd\n",
    "import json # For handling JSON data\n",
    "from ucimlrepo import fetch_ucirepo # To get the dataset\n",
    "\n",
    "# --- Start a new MLflow run for this data loading & prep phase ---\n",
    "# (This assumes your MLFLOW_TRACKING_URI, USERNAME, PASSWORD are set via os.environ\n",
    "# and mlflow.set_experiment() has been called from the previous setup cell)\n",
    "\n",
    "# If a run is already active from a previous cell, you might want to end it first\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    print(\"Ended previous active run.\")\n",
    "\n",
    "# Start a new run for data ingestion\n",
    "data_ingestion_run = mlflow.start_run(run_name=\"Data_Ingestion_and_Initial_Prep\")\n",
    "print(f\"Starting new MLflow Run for Data Ingestion: {data_ingestion_run.data.tags.get('mlflow.runName')}\")\n",
    "print(f\"Run ID: {data_ingestion_run.info.run_id}\")\n",
    "\n",
    "# --- 1. Fetch Dataset ---\n",
    "print(\"Fetching UCI Adult dataset...\")\n",
    "try:\n",
    "    adult_dataset_info = fetch_ucirepo(id=2) # id=2 is for the Adult dataset\n",
    "    X_raw = adult_dataset_info.data.features\n",
    "    y_raw = adult_dataset_info.data.targets\n",
    "    print(\"Dataset fetched successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching dataset: {e}\")\n",
    "    mlflow.log_param(\"data_fetching_status\", \"failed\")\n",
    "    mlflow.log_param(\"data_fetching_error\", str(e))\n",
    "    if mlflow.active_run(): mlflow.end_run() # End run if fetching fails\n",
    "    raise # Re-raise the exception to stop execution\n",
    "\n",
    "# --- 2. Combine Features and Target ---\n",
    "# For easier handling, let's put features and target into one DataFrame\n",
    "df_raw = pd.concat([X_raw, y_raw], axis=1)\n",
    "print(\"\\nCombined DataFrame head (first 5 rows):\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# --- 3. Basic Data Type Conversion ---\n",
    "# Some integer columns might be better as floats for later processing\n",
    "# or to handle potential NaNs more consistently.\n",
    "int_columns = df_raw.select_dtypes(include='int64').columns\n",
    "if not int_columns.empty:\n",
    "    df_raw[int_columns] = df_raw[int_columns].astype('float64')\n",
    "    print(f\"\\nConverted integer columns to float: {list(int_columns)}\")\n",
    "    mlflow.log_param(\"int_columns_converted_to_float\", list(int_columns))\n",
    "else:\n",
    "    print(\"\\nNo int64 columns found to convert.\")\n",
    "    mlflow.log_param(\"int_columns_converted_to_float\", \"None\")\n",
    "\n",
    "target_column_name = y_raw.columns[0] # Get the target column name (e.g., 'income')\n",
    "\n",
    "# --- 4. Logging Dataset as an MLflow Input ---\n",
    "# MLflow can track datasets as inputs, which helps with lineage.\n",
    "print(f\"\\nLogging dataset as MLflow input. Target column: '{target_column_name}'\")\n",
    "try:\n",
    "    mlflow_dataset = mlflow.data.from_pandas(\n",
    "        df_raw,\n",
    "        source=adult_dataset_info.metadata.get('data_url', 'UCI Repository ID 2'),\n",
    "        name=\"UCI Adult Income - Raw Combined\",\n",
    "        targets=target_column_name\n",
    "    )\n",
    "    mlflow.log_input(mlflow_dataset, context=\"raw_dataset\") # context is a tag\n",
    "    print(\"Dataset logged as MLflow input.\")\n",
    "    mlflow.log_param(\"raw_dataset_logged_as_input\", \"success\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging dataset as MLflow input: {e}\")\n",
    "    mlflow.log_param(\"raw_dataset_logged_as_input\", \"failed\")\n",
    "    mlflow.log_param(\"raw_dataset_logging_error\", str(e))\n",
    "\n",
    "\n",
    "# --- 5. Save and Log Dataset Metadata as Artifacts ---\n",
    "# The fetched dataset has metadata. Let's save this as a JSON artifact.\n",
    "# This is handy for understanding where the data came from, variable descriptions, etc.\n",
    "\n",
    "# Create a local directory to temporarily store artifacts before logging\n",
    "# This is good practice if you have multiple files or want to organize them.\n",
    "local_artifacts_dir = \"temp_data_ingestion_artifacts\"\n",
    "os.makedirs(local_artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Save original UCI metadata\n",
    "uci_metadata_file_path = os.path.join(local_artifacts_dir, \"uci_adult_dataset_metadata.json\")\n",
    "try:\n",
    "    with open(uci_metadata_file_path, \"w\") as f:\n",
    "        json.dump(adult_dataset_info.metadata, f, indent=4)\n",
    "    mlflow.log_artifact(uci_metadata_file_path, artifact_path=\"dataset_description\")\n",
    "    print(f\"\\nUCI dataset metadata logged to MLflow artifacts: {uci_metadata_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging UCI metadata: {e}\")\n",
    "\n",
    "# Save variable information from UCI\n",
    "uci_variables_file_path = os.path.join(local_artifacts_dir, \"uci_adult_dataset_variables.json\")\n",
    "try:\n",
    "    # Convert DataFrame to dict for JSON serialization if it's a DataFrame\n",
    "    variables_info_serializable = adult_dataset_info.variables.to_dict(orient='records') if isinstance(adult_dataset_info.variables, pd.DataFrame) else adult_dataset_info.variables\n",
    "    with open(uci_variables_file_path, \"w\") as f:\n",
    "        json.dump(variables_info_serializable, f, indent=4)\n",
    "    mlflow.log_artifact(uci_variables_file_path, artifact_path=\"dataset_description\")\n",
    "    print(f\"UCI dataset variable info logged to MLflow artifacts: {uci_variables_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging UCI variable info: {e}\")\n",
    "\n",
    "\n",
    "# --- 6. Extract and Log Unique Values for Categorical Features ---\n",
    "# Knowing unique values in categorical columns is key for EDA and preprocessing.\n",
    "unique_values_categorical = {}\n",
    "# Select columns that are 'object' (strings) or 'category' type\n",
    "categorical_cols = df_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Make sure the target column isn't accidentally included if it's an object type\n",
    "if target_column_name in categorical_cols:\n",
    "    categorical_cols.remove(target_column_name)\n",
    "    \n",
    "for col in categorical_cols:\n",
    "    # df_raw[col].dropna().unique() ensures we don't include NaN if it's treated as a category\n",
    "    unique_values_categorical[col] = [str(val) for val in df_raw[col].dropna().unique().tolist()]\n",
    "\n",
    "\n",
    "unique_values_file_path = os.path.join(local_artifacts_dir, \"categorical_features_unique_values.json\")\n",
    "try:\n",
    "    with open(unique_values_file_path, \"w\") as f:\n",
    "        json.dump(unique_values_categorical, f, indent=4)\n",
    "    mlflow.log_artifact(unique_values_file_path, artifact_path=\"dataset_description\")\n",
    "    print(f\"Unique values for categorical features logged to MLflow artifacts: {unique_values_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging unique categorical values: {e}\")\n",
    "\n",
    "# --- 7. Log Dataset Overview Parameters ---\n",
    "# Let's log some general stats about the dataset as MLflow parameters.\n",
    "# This gives a quick overview in the MLflow UI for this run.\n",
    "print(\"\\nLogging dataset overview parameters to MLflow...\")\n",
    "try:\n",
    "    dataset_params = {\n",
    "        \"dataset_name\": \"UCI Adult Income\",\n",
    "        \"source_repository_id\": adult_dataset_info.metadata.get('uci_id', 2),\n",
    "        \"num_rows_raw\": df_raw.shape[0],\n",
    "        \"num_columns_raw_total\": df_raw.shape[1],\n",
    "        \"num_features_raw\": X_raw.shape[1],\n",
    "        \"num_target_columns_raw\": y_raw.shape[1],\n",
    "        \"column_names_raw\": df_raw.columns.tolist(),\n",
    "        \"numerical_columns_count_raw\": df_raw.select_dtypes(include='number').shape[1],\n",
    "        \"categorical_columns_count_raw\": len(categorical_cols),\n",
    "        \"categorical_column_names_raw\": categorical_cols,\n",
    "        \"numerical_column_names_raw\": df_raw.select_dtypes(include='number').columns.tolist(),\n",
    "        \"missing_values_total_raw\": int(df_raw.isnull().sum().sum()),\n",
    "        \"target_column_name\": target_column_name,\n",
    "        \"target_unique_values_count\": df_raw[target_column_name].nunique(),\n",
    "        \"target_unique_values_list\": [str(val) for val in df_raw[target_column_name].unique().tolist()],\n",
    "        \"target_value_counts\": {str(k): v for k, v in df_raw[target_column_name].value_counts().to_dict().items()},\n",
    "        \"target_value_counts_percentage\": {str(k): v for k, v in df_raw[target_column_name].value_counts(normalize=True).to_dict().items()}\n",
    "    }\n",
    "    # MLflow parameters have a length limit (often 250 chars for value), so for long lists, consider logging as a text artifact.\n",
    "    # For this example, we'll try logging directly.\n",
    "    for key, value in dataset_params.items():\n",
    "        if isinstance(value, (list, dict)):\n",
    "            # Convert lists/dicts to string for parameters, or log them as JSON artifacts if too long/complex\n",
    "            try:\n",
    "                mlflow.log_param(key, json.dumps(value))\n",
    "            except TypeError: # handles cases where json.dumps might fail for complex objects\n",
    "                 mlflow.log_param(key, str(value))\n",
    "\n",
    "        else:\n",
    "            mlflow.log_param(key, value)\n",
    "    print(\"Dataset overview parameters logged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging dataset parameters: {e}\")\n",
    "\n",
    "\n",
    "# --- 8. Log a Sample of the Raw Data as an Artifact (e.g., CSV) ---\n",
    "# This can be useful for quick inspection from the MLflow UI.\n",
    "# Be mindful of data size; log a small sample if the dataset is large.\n",
    "sample_df_path = os.path.join(local_artifacts_dir, \"raw_data_sample.csv\")\n",
    "try:\n",
    "    df_raw.head(100).to_csv(sample_df_path, index=False) # Log first 100 rows\n",
    "    mlflow.log_artifact(sample_df_path, artifact_path=\"dataset_samples\")\n",
    "    print(f\"Logged a sample of raw data to MLflow artifacts: {sample_df_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging data sample: {e}\")\n",
    "\n",
    "# Clean up temporary local artifacts directory (optional)\n",
    "# import shutil\n",
    "# shutil.rmtree(local_artifacts_dir)\n",
    "# print(f\"Removed temporary local artifacts directory: {local_artifacts_dir}\")\n",
    "\n",
    "print(f\"\\nData ingestion and initial preparation run finished. Check MLflow UI for run ID: {data_ingestion_run.info.run_id}\")\n",
    "\n",
    "# --- End the current MLflow run ---\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    print(\"MLflow run for Data Ingestion ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff5020",
   "metadata": {},
   "source": [
    "# MLflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204edb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "MLFLOW_USERNAME = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "MLFLOW_PASSWORD = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.start_run(run_name=\"EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5308d",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebab4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether annual income of an individual exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Tue Sep 24 2024', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': \"Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nPrediction task is to determine whether a person's income is over $50,000 a year.\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(adult.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52ed42",
   "metadata": {},
   "source": [
    "# Data Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b438a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample after combining X and y:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "workclass",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "fnlwgt",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "education",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "education-num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "marital-status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "occupation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "relationship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "race",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sex",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capital-gain",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "capital-loss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hours-per-week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "native-country",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "income",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "63cc2684-89c6-4a84-b94d-0902f31ad204",
       "rows": [
        [
         "0",
         "39",
         "State-gov",
         "77516",
         "Bachelors",
         "13",
         "Never-married",
         "Adm-clerical",
         "Not-in-family",
         "White",
         "Male",
         "2174",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "1",
         "50",
         "Self-emp-not-inc",
         "83311",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Exec-managerial",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "13",
         "United-States",
         "<=50K"
        ],
        [
         "2",
         "38",
         "Private",
         "215646",
         "HS-grad",
         "9",
         "Divorced",
         "Handlers-cleaners",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "3",
         "53",
         "Private",
         "234721",
         "11th",
         "7",
         "Married-civ-spouse",
         "Handlers-cleaners",
         "Husband",
         "Black",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "4",
         "28",
         "Private",
         "338409",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Prof-specialty",
         "Wife",
         "Black",
         "Female",
         "0",
         "0",
         "40",
         "Cuba",
         "<=50K"
        ],
        [
         "5",
         "37",
         "Private",
         "284582",
         "Masters",
         "14",
         "Married-civ-spouse",
         "Exec-managerial",
         "Wife",
         "White",
         "Female",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "6",
         "49",
         "Private",
         "160187",
         "9th",
         "5",
         "Married-spouse-absent",
         "Other-service",
         "Not-in-family",
         "Black",
         "Female",
         "0",
         "0",
         "16",
         "Jamaica",
         "<=50K"
        ],
        [
         "7",
         "52",
         "Self-emp-not-inc",
         "209642",
         "HS-grad",
         "9",
         "Married-civ-spouse",
         "Exec-managerial",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "45",
         "United-States",
         ">50K"
        ],
        [
         "8",
         "31",
         "Private",
         "45781",
         "Masters",
         "14",
         "Never-married",
         "Prof-specialty",
         "Not-in-family",
         "White",
         "Female",
         "14084",
         "0",
         "50",
         "United-States",
         ">50K"
        ],
        [
         "9",
         "42",
         "Private",
         "159449",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Exec-managerial",
         "Husband",
         "White",
         "Male",
         "5178",
         "0",
         "40",
         "United-States",
         ">50K"
        ],
        [
         "10",
         "37",
         "Private",
         "280464",
         "Some-college",
         "10",
         "Married-civ-spouse",
         "Exec-managerial",
         "Husband",
         "Black",
         "Male",
         "0",
         "0",
         "80",
         "United-States",
         ">50K"
        ],
        [
         "11",
         "30",
         "State-gov",
         "141297",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Prof-specialty",
         "Husband",
         "Asian-Pac-Islander",
         "Male",
         "0",
         "0",
         "40",
         "India",
         ">50K"
        ],
        [
         "12",
         "23",
         "Private",
         "122272",
         "Bachelors",
         "13",
         "Never-married",
         "Adm-clerical",
         "Own-child",
         "White",
         "Female",
         "0",
         "0",
         "30",
         "United-States",
         "<=50K"
        ],
        [
         "13",
         "32",
         "Private",
         "205019",
         "Assoc-acdm",
         "12",
         "Never-married",
         "Sales",
         "Not-in-family",
         "Black",
         "Male",
         "0",
         "0",
         "50",
         "United-States",
         "<=50K"
        ],
        [
         "14",
         "40",
         "Private",
         "121772",
         "Assoc-voc",
         "11",
         "Married-civ-spouse",
         "Craft-repair",
         "Husband",
         "Asian-Pac-Islander",
         "Male",
         "0",
         "0",
         "40",
         "?",
         ">50K"
        ],
        [
         "15",
         "34",
         "Private",
         "245487",
         "7th-8th",
         "4",
         "Married-civ-spouse",
         "Transport-moving",
         "Husband",
         "Amer-Indian-Eskimo",
         "Male",
         "0",
         "0",
         "45",
         "Mexico",
         "<=50K"
        ],
        [
         "16",
         "25",
         "Self-emp-not-inc",
         "176756",
         "HS-grad",
         "9",
         "Never-married",
         "Farming-fishing",
         "Own-child",
         "White",
         "Male",
         "0",
         "0",
         "35",
         "United-States",
         "<=50K"
        ],
        [
         "17",
         "32",
         "Private",
         "186824",
         "HS-grad",
         "9",
         "Never-married",
         "Machine-op-inspct",
         "Unmarried",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "18",
         "38",
         "Private",
         "28887",
         "11th",
         "7",
         "Married-civ-spouse",
         "Sales",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "50",
         "United-States",
         "<=50K"
        ],
        [
         "19",
         "43",
         "Self-emp-not-inc",
         "292175",
         "Masters",
         "14",
         "Divorced",
         "Exec-managerial",
         "Unmarried",
         "White",
         "Female",
         "0",
         "0",
         "45",
         "United-States",
         ">50K"
        ],
        [
         "20",
         "40",
         "Private",
         "193524",
         "Doctorate",
         "16",
         "Married-civ-spouse",
         "Prof-specialty",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "60",
         "United-States",
         ">50K"
        ],
        [
         "21",
         "54",
         "Private",
         "302146",
         "HS-grad",
         "9",
         "Separated",
         "Other-service",
         "Unmarried",
         "Black",
         "Female",
         "0",
         "0",
         "20",
         "United-States",
         "<=50K"
        ],
        [
         "22",
         "35",
         "Federal-gov",
         "76845",
         "9th",
         "5",
         "Married-civ-spouse",
         "Farming-fishing",
         "Husband",
         "Black",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "23",
         "43",
         "Private",
         "117037",
         "11th",
         "7",
         "Married-civ-spouse",
         "Transport-moving",
         "Husband",
         "White",
         "Male",
         "0",
         "2042",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "24",
         "59",
         "Private",
         "109015",
         "HS-grad",
         "9",
         "Divorced",
         "Tech-support",
         "Unmarried",
         "White",
         "Female",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "25",
         "56",
         "Local-gov",
         "216851",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Tech-support",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         ">50K"
        ],
        [
         "26",
         "19",
         "Private",
         "168294",
         "HS-grad",
         "9",
         "Never-married",
         "Craft-repair",
         "Own-child",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "27",
         "54",
         "?",
         "180211",
         "Some-college",
         "10",
         "Married-civ-spouse",
         "?",
         "Husband",
         "Asian-Pac-Islander",
         "Male",
         "0",
         "0",
         "60",
         "South",
         ">50K"
        ],
        [
         "28",
         "39",
         "Private",
         "367260",
         "HS-grad",
         "9",
         "Divorced",
         "Exec-managerial",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "80",
         "United-States",
         "<=50K"
        ],
        [
         "29",
         "49",
         "Private",
         "193366",
         "HS-grad",
         "9",
         "Married-civ-spouse",
         "Craft-repair",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "30",
         "23",
         "Local-gov",
         "190709",
         "Assoc-acdm",
         "12",
         "Never-married",
         "Protective-serv",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "52",
         "United-States",
         "<=50K"
        ],
        [
         "31",
         "20",
         "Private",
         "266015",
         "Some-college",
         "10",
         "Never-married",
         "Sales",
         "Own-child",
         "Black",
         "Male",
         "0",
         "0",
         "44",
         "United-States",
         "<=50K"
        ],
        [
         "32",
         "45",
         "Private",
         "386940",
         "Bachelors",
         "13",
         "Divorced",
         "Exec-managerial",
         "Own-child",
         "White",
         "Male",
         "0",
         "1408",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "33",
         "30",
         "Federal-gov",
         "59951",
         "Some-college",
         "10",
         "Married-civ-spouse",
         "Adm-clerical",
         "Own-child",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "34",
         "22",
         "State-gov",
         "311512",
         "Some-college",
         "10",
         "Married-civ-spouse",
         "Other-service",
         "Husband",
         "Black",
         "Male",
         "0",
         "0",
         "15",
         "United-States",
         "<=50K"
        ],
        [
         "35",
         "48",
         "Private",
         "242406",
         "11th",
         "7",
         "Never-married",
         "Machine-op-inspct",
         "Unmarried",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "Puerto-Rico",
         "<=50K"
        ],
        [
         "36",
         "21",
         "Private",
         "197200",
         "Some-college",
         "10",
         "Never-married",
         "Machine-op-inspct",
         "Own-child",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "37",
         "19",
         "Private",
         "544091",
         "HS-grad",
         "9",
         "Married-AF-spouse",
         "Adm-clerical",
         "Wife",
         "White",
         "Female",
         "0",
         "0",
         "25",
         "United-States",
         "<=50K"
        ],
        [
         "38",
         "31",
         "Private",
         "84154",
         "Some-college",
         "10",
         "Married-civ-spouse",
         "Sales",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "38",
         "?",
         ">50K"
        ],
        [
         "39",
         "48",
         "Self-emp-not-inc",
         "265477",
         "Assoc-acdm",
         "12",
         "Married-civ-spouse",
         "Prof-specialty",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "40",
         "31",
         "Private",
         "507875",
         "9th",
         "5",
         "Married-civ-spouse",
         "Machine-op-inspct",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "43",
         "United-States",
         "<=50K"
        ],
        [
         "41",
         "53",
         "Self-emp-not-inc",
         "88506",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Prof-specialty",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "42",
         "24",
         "Private",
         "172987",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Tech-support",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "50",
         "United-States",
         "<=50K"
        ],
        [
         "43",
         "49",
         "Private",
         "94638",
         "HS-grad",
         "9",
         "Separated",
         "Adm-clerical",
         "Unmarried",
         "White",
         "Female",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "44",
         "25",
         "Private",
         "289980",
         "HS-grad",
         "9",
         "Never-married",
         "Handlers-cleaners",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "35",
         "United-States",
         "<=50K"
        ],
        [
         "45",
         "57",
         "Federal-gov",
         "337895",
         "Bachelors",
         "13",
         "Married-civ-spouse",
         "Prof-specialty",
         "Husband",
         "Black",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         ">50K"
        ],
        [
         "46",
         "53",
         "Private",
         "144361",
         "HS-grad",
         "9",
         "Married-civ-spouse",
         "Machine-op-inspct",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "38",
         "United-States",
         "<=50K"
        ],
        [
         "47",
         "44",
         "Private",
         "128354",
         "Masters",
         "14",
         "Divorced",
         "Exec-managerial",
         "Unmarried",
         "White",
         "Female",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "48",
         "41",
         "State-gov",
         "101603",
         "Assoc-voc",
         "11",
         "Married-civ-spouse",
         "Craft-repair",
         "Husband",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States",
         "<=50K"
        ],
        [
         "49",
         "29",
         "Private",
         "271466",
         "Assoc-voc",
         "11",
         "Never-married",
         "Prof-specialty",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "43",
         "United-States",
         "<=50K"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 48842
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64               NaN  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                NaN  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  income  \n",
       "0      United-States   <=50K  \n",
       "1      United-States   <=50K  \n",
       "2      United-States   <=50K  \n",
       "3      United-States   <=50K  \n",
       "4               Cuba   <=50K  \n",
       "...              ...     ...  \n",
       "48837  United-States  <=50K.  \n",
       "48838  United-States  <=50K.  \n",
       "48839  United-States  <=50K.  \n",
       "48840  United-States  <=50K.  \n",
       "48841  United-States   >50K.  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine features and target into one DataFrame\n",
    "df = pd.concat([X, y], axis=1)\n",
    "print(\"Data sample after combining X and y:\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0c4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education-num      int64\n",
       "marital-status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital-gain       int64\n",
       "capital-loss       int64\n",
       "hours-per-week     int64\n",
       "native-country    object\n",
       "income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ce414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = df.select_dtypes(include='int64').columns\n",
    "df[int_columns] = df[int_columns].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8eff46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               float64\n",
       "workclass          object\n",
       "fnlwgt            float64\n",
       "education          object\n",
       "education-num     float64\n",
       "marital-status     object\n",
       "occupation         object\n",
       "relationship       object\n",
       "race               object\n",
       "sex                object\n",
       "capital-gain      float64\n",
       "capital-loss      float64\n",
       "hours-per-week    float64\n",
       "native-country     object\n",
       "income             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9a6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = y.columns[0]\n",
    "\n",
    "# Create a Dataset object from the DataFrame\n",
    "dataset = mlflow.data.from_pandas(df, name=\"UCI Adult Income\", targets=target_column)\n",
    "\n",
    "mlflow.log_input(dataset, context=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11ee07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the directory name\n",
    "metadata_dir = \"../data/metadata\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "metadata_file_path = os.path.join(metadata_dir, \"adult_metadata.json\")\n",
    "with open(metadata_file_path, \"w\") as f:\n",
    "    json.dump(adult.metadata, f, indent=2)\n",
    "\n",
    "\n",
    "unique_values = {}\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    unique_values[col] = df[col].dropna().unique().tolist()\n",
    "\n",
    "\n",
    "unique_values_file_path = os.path.join(metadata_dir, \"unique_values.json\")\n",
    "with open(unique_values_file_path, \"w\") as f:\n",
    "    json.dump(unique_values, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15a601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_artifact(unique_values_file_path, artifact_path=\"metadata\")\n",
    "mlflow.log_artifact(metadata_file_path, artifact_path=\"metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda3868",
   "metadata": {},
   "source": [
    "# Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e40af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (48842, 15)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             48842 non-null  float64\n",
      " 1   workclass       47879 non-null  object \n",
      " 2   fnlwgt          48842 non-null  float64\n",
      " 3   education       48842 non-null  object \n",
      " 4   education-num   48842 non-null  float64\n",
      " 5   marital-status  48842 non-null  object \n",
      " 6   occupation      47876 non-null  object \n",
      " 7   relationship    48842 non-null  object \n",
      " 8   race            48842 non-null  object \n",
      " 9   sex             48842 non-null  object \n",
      " 10  capital-gain    48842 non-null  float64\n",
      " 11  capital-loss    48842 non-null  float64\n",
      " 12  hours-per-week  48842 non-null  float64\n",
      " 13  native-country  48568 non-null  object \n",
      " 14  income          48842 non-null  object \n",
      "dtypes: float64(6), object(9)\n",
      "memory usage: 5.6+ MB\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  48842.000000  4.884200e+04   48842.000000  48842.000000  48842.000000   \n",
      "mean      38.643585  1.896641e+05      10.078089   1079.067626     87.502314   \n",
      "std       13.710510  1.056040e+05       2.570973   7452.019058    403.004552   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.175505e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.781445e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.376420e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    48842.000000  \n",
      "mean        40.422382  \n",
      "std         12.391444  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "Unique values for workclass: ['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' '?'\n",
      " 'Self-emp-inc' 'Without-pay' 'Never-worked' nan]\n",
      "Unique values for education: ['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n",
      " 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n",
      " '1st-4th' 'Preschool' '12th']\n",
      "Unique values for marital-status: ['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent'\n",
      " 'Separated' 'Married-AF-spouse' 'Widowed']\n",
      "Unique values for occupation: ['Adm-clerical' 'Exec-managerial' 'Handlers-cleaners' 'Prof-specialty'\n",
      " 'Other-service' 'Sales' 'Craft-repair' 'Transport-moving'\n",
      " 'Farming-fishing' 'Machine-op-inspct' 'Tech-support' '?'\n",
      " 'Protective-serv' 'Armed-Forces' 'Priv-house-serv' nan]\n",
      "Unique values for relationship: ['Not-in-family' 'Husband' 'Wife' 'Own-child' 'Unmarried' 'Other-relative']\n",
      "Unique values for race: ['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other']\n",
      "Unique values for sex: ['Male' 'Female']\n",
      "Unique values for native-country: ['United-States' 'Cuba' 'Jamaica' 'India' '?' 'Mexico' 'South'\n",
      " 'Puerto-Rico' 'Honduras' 'England' 'Canada' 'Germany' 'Iran'\n",
      " 'Philippines' 'Italy' 'Poland' 'Columbia' 'Cambodia' 'Thailand' 'Ecuador'\n",
      " 'Laos' 'Taiwan' 'Haiti' 'Portugal' 'Dominican-Republic' 'El-Salvador'\n",
      " 'France' 'Guatemala' 'China' 'Japan' 'Yugoslavia' 'Peru'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'Scotland' 'Trinadad&Tobago' 'Greece'\n",
      " 'Nicaragua' 'Vietnam' 'Hong' 'Ireland' 'Hungary' 'Holand-Netherlands' nan]\n",
      "Unique values for income: ['<=50K' '>50K' '<=50K.' '>50K.']\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions and basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.info()\n",
    "print(df.describe())\n",
    "\n",
    "# For categorical columns, you can view unique values as well\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"Unique values for {col}: {df[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751ea0f",
   "metadata": {},
   "source": [
    "### üîç Problem:\n",
    "```\n",
    "['<=50K', '>50K', '<=50K.', '>50K.']\n",
    "```\n",
    "\n",
    "This means the **same labels** appear with and without a **trailing period (`.`)**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Cause:\n",
    "The UCI Adult dataset comes in two files:\n",
    "- **`adult.data`** (no headers, training data)\n",
    "- **`adult.test`** (starts with a header/comment line, test data)\n",
    "\n",
    "In the `adult.test` file, **labels have a period** at the end ‚Äî i.e., `'>50K.'` and '`<=50K.'`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Why It Matters:\n",
    "- Your model might treat `'<=50K'` and '`<=50K.'` as **different classes**, which leads to:\n",
    "  - Incorrect label counts\n",
    "  - Misleading model evaluation\n",
    "  - Skewed visualizations\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution:\n",
    "Standardize the labels early during preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a217fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace and trailing periods from income\n",
    "df['income'] = df['income'].str.strip().str.replace('.', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907d094",
   "metadata": {},
   "source": [
    "# Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a211af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count:\n",
      "age                  0\n",
      "workclass         2799\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        2809\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     857\n",
      "income               0\n",
      "dtype: int64\n",
      "Missing values percentage per column:\n",
      "age               0.000000\n",
      "workclass         5.730724\n",
      "fnlwgt            0.000000\n",
      "education         0.000000\n",
      "education-num     0.000000\n",
      "marital-status    0.000000\n",
      "occupation        5.751198\n",
      "relationship      0.000000\n",
      "race              0.000000\n",
      "sex               0.000000\n",
      "capital-gain      0.000000\n",
      "capital-loss      0.000000\n",
      "hours-per-week    0.000000\n",
      "native-country    1.754637\n",
      "income            0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace '?' with np.nan for consistent missing value notation\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Check the number of missing values per column\n",
    "print(\"Missing values count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Calculate percentage of missing values (optional)\n",
    "missing_percent = df.isnull().mean() * 100\n",
    "print(\"Missing values percentage per column:\")\n",
    "print(missing_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2eb30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/56hv4rgj2yn_2z_m3dwvcwhc0000gn/T/ipykernel_3113/2383048195.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_cols:\n",
    "    df[col].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcdced",
   "metadata": {},
   "source": [
    "## Display and log dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5b9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log basic info\n",
    "mlflow.log_params({\n",
    "    \"dataset_name\": \"adult\",\n",
    "    \"no_of_cols\": df.shape[1],\n",
    "    \"no_of_rows\": df.shape[0],\n",
    "    \"columns\": ','.join(df.columns.tolist()),\n",
    "    \"numerical_columns_count\": df.select_dtypes(include='number').shape[1],\n",
    "    \"missing_values_total\": int(df.isnull().sum().sum()),\n",
    "    \"target_column\": \"income\",\n",
    "    \"categorical_columns_count\": len(categorical_cols),\n",
    "    \"categorical_columns\": ','.join(categorical_cols),\n",
    "    \"numerical_columns\": ','.join(df.select_dtypes(include='number').columns.tolist()),\n",
    "    \"target_column_unique_values\": df['income'].nunique(),\n",
    "    \"target_column_unique_values_list\": ','.join(df['income'].unique().tolist()),\n",
    "    \"target_column_value_counts\": df['income'].value_counts().to_dict(),\n",
    "    \"target_column_value_counts_percentage\": df['income'].value_counts(normalize=True).to_dict()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65016e",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA) with MLflow Artifact Logging\n",
    "\n",
    "Exploratory Data Analysis is a crucial step to understand the dataset's characteristics, distributions, correlations, and potential issues. A key part of EDA involves generating visualizations. Traditionally, these plots might live temporarily in a notebook's output or be manually saved to local folders. With MLflow, we can automatically log these plots as artifacts directly associated with a specific experiment run, preserving the visual insights alongside the code and parameters that generated them.\n",
    "\n",
    "Let's start a new MLflow run specifically for our EDA phase. This helps keep the logs organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b01abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run EDA at: http://135.235.251.124/#/experiments/2/runs/806585c8a8d545e191b53fb80668533a\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "Ended previous active run.\n",
      "Starting new MLflow Run for EDA: Exploratory_Data_Analysis\n",
      "Run ID: 0ad8a52b62274b879be38a9253ed0923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'raw_combined'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Notebook Cell: Start EDA Run ---\n",
    "# Ensure MLflow tracking URI and credentials are set from the initial setup cell\n",
    "# Assumes 'df_raw' DataFrame is available from the previous Data Ingestion phase.\n",
    "# Assumes 'target_column_name' and 'categorical_cols' were defined earlier.\n",
    "\n",
    "# If a previous run is active, end it first.\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    print(\"Ended previous active run.\")\n",
    "\n",
    "# Start a new run dedicated to EDA\n",
    "eda_run = mlflow.start_run(run_name=\"Exploratory_Data_Analysis\")\n",
    "print(f\"Starting new MLflow Run for EDA: {eda_run.data.tags.get('mlflow.runName')}\")\n",
    "print(f\"Run ID: {eda_run.info.run_id}\")\n",
    "\n",
    "# For convenience, let's use a shorter variable name for our DataFrame in this phase\n",
    "df = df_raw # Or df = df_cleaned if you performed cleaning steps\n",
    "\n",
    "# Log a parameter indicating the data state used for EDA\n",
    "mlflow.log_param(\"eda_data_source\", \"raw_combined\") # Or \"cleaned\" if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c78fc",
   "metadata": {},
   "source": [
    "Now, let's generate various plots and log them using `mlflow.log_figure()`. This function takes the current Matplotlib figure (`plt.gcf()`) and saves it as an image artifact to the specified path within the run's artifact store (our Azure Blob Storage).\n",
    "\n",
    "## **1. Visualizing Numerical Feature Distributions (Histograms)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a229b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging histograms for numerical features...\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_age.png\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_fnlwgt.png\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_education-num.png\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_capital-gain.png\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_capital-loss.png\n",
      "  Logged histogram: eda_plots/numerical_distributions/hist_hours-per-week.png\n",
      "Finished logging histograms.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Numerical Histograms ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "\n",
    "print(\"Generating and logging histograms for numerical features...\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Select only numerical columns (excluding potential IDs if necessary)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "mlflow.log_param(\"numerical_features_for_eda\", list(numerical_cols))\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], bins=30, kde=True, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log the figure directly to MLflow under a specific sub-directory\n",
    "    # Using f-strings makes organizing artifacts easy\n",
    "    artifact_path = f\"eda_plots/numerical_distributions/hist_{col}.png\"\n",
    "    mlflow.log_figure(plt.gcf(), artifact_path)\n",
    "    print(f\"  Logged histogram: {artifact_path}\")\n",
    "\n",
    "    # plt.show() # Display the plot in the notebook (optional)\n",
    "    plt.close() # Close the figure to free memory, crucial in loops!\n",
    "\n",
    "print(\"Finished logging histograms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965435",
   "metadata": {},
   "source": [
    "*   **`mlflow.log_figure(plt.gcf(), artifact_path)`**: This is the core command. `plt.gcf()` gets the current Matplotlib figure, and `artifact_path` defines where it will be stored within the MLflow run's artifacts (e.g., `eda_plots/numerical_distributions/hist_age.png`).\n",
    "*   **`plt.close()`**: Closing the figure after logging is important, especially within loops, to prevent plots from consuming excessive memory or interfering with subsequent plots.\n",
    "\n",
    "## **2. Visualizing Numerical Feature Distributions (Boxplots)**\n",
    "\n",
    "Boxplots help identify outliers and understand the spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7a9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging boxplots for numerical features...\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_age.png\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_fnlwgt.png\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_education-num.png\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_capital-gain.png\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_capital-loss.png\n",
      "  Logged boxplot: eda_plots/numerical_distributions/boxplot_hours-per-week.png\n",
      "Finished logging boxplots.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Numerical Boxplots ---\n",
    "print(\"Generating and logging boxplots for numerical features...\")\n",
    "\n",
    "for col in numerical_cols: # Reuse numerical_cols from previous cell\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df[col], color='lightgreen', linewidth=1.5)\n",
    "    plt.title(f\"Boxplot for {col}\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log the figure directly to MLflow\n",
    "    artifact_path = f\"eda_plots/numerical_distributions/boxplot_{col}.png\"\n",
    "    mlflow.log_figure(plt.gcf(), artifact_path)\n",
    "    print(f\"  Logged boxplot: {artifact_path}\")\n",
    "\n",
    "    # plt.show() # Optional display\n",
    "    plt.close()\n",
    "\n",
    "print(\"Finished logging boxplots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f295c7",
   "metadata": {},
   "source": [
    "## **3. Visualizing Numerical Feature Correlations (Heatmap)**\n",
    "\n",
    "Understanding correlations is vital for feature selection and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfaefcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging correlation heatmap...\n",
      "  Logged heatmap: eda_plots/correlation_heatmap.png\n",
      "Finished logging correlation heatmap.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Correlation Heatmap ---\n",
    "print(\"Generating and logging correlation heatmap...\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Recalculate correlation matrix on the numerical columns identified\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "sns.heatmap(\n",
    "    corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5,\n",
    "    square=True, cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Log the figure directly to MLflow\n",
    "artifact_path = \"eda_plots/correlation_heatmap.png\"\n",
    "mlflow.log_figure(plt.gcf(), artifact_path)\n",
    "print(f\"  Logged heatmap: {artifact_path}\")\n",
    "\n",
    "# plt.show() # Optional display\n",
    "plt.close()\n",
    "\n",
    "print(\"Finished logging correlation heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ff6aa",
   "metadata": {},
   "source": [
    "## **4. Visualizing Categorical Feature Distributions**\n",
    "\n",
    "Let's see the counts for each category in our categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b531ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging distributions for categorical features...\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_workclass.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_education.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_marital-status.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_occupation.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_relationship.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_race.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_sex.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_native-country.png\n",
      "  Logged categorical distribution: eda_plots/categorical_distributions/dist_income.png\n",
      "Finished logging categorical distributions.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Categorical Distributions ---\n",
    "import matplotlib.pyplot as plt # Ensure imported if in new session\n",
    "import seaborn as sns         # Ensure imported\n",
    "import mlflow               # Ensure imported\n",
    "\n",
    "print(\"Generating and logging distributions for categorical features...\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Ensure 'categorical_cols' is defined (likely from data ingestion phase)\n",
    "# Example: categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Make sure the target column isn't included if it's categorical\n",
    "# if target_column_name in categorical_cols:\n",
    "#     categorical_cols.remove(target_column_name)\n",
    "\n",
    "mlflow.log_param(\"categorical_features_for_eda\", list(categorical_cols))\n",
    "\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Using hue=col helps if colors are needed per bar, otherwise optional\n",
    "    sns.countplot(\n",
    "        data=df,\n",
    "        y=col, # Using y can be better for long category names\n",
    "        order=df[col].value_counts().index, # Order by frequency\n",
    "        palette=\"viridis\",\n",
    "        hue=col, # Assign hue for color mapping, necessary if legend=False fails\n",
    "        legend=False # Avoid redundant legend if hue=col\n",
    "    )\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Count\", fontsize=12)\n",
    "    plt.ylabel(col, fontsize=12)\n",
    "    # plt.xticks(rotation=45) # Not needed if using y=col\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log the figure directly to MLflow\n",
    "    artifact_path = f\"eda_plots/categorical_distributions/dist_{col}.png\"\n",
    "    mlflow.log_figure(plt.gcf(), artifact_path)\n",
    "    print(f\"  Logged categorical distribution: {artifact_path}\")\n",
    "\n",
    "    # plt.show() # Optional display\n",
    "    plt.close()\n",
    "\n",
    "print(\"Finished logging categorical distributions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237bbb2",
   "metadata": {},
   "source": [
    "## **5. Visualizing Categorical Features vs. Target Variable**\n",
    "\n",
    "Understanding how categorical features relate to the target variable (`income`) is key for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d4bfbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging categorical features vs target...\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/workclass_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/education_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/marital-status_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/occupation_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/relationship_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/race_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/sex_vs_income.png\n",
      "  Logged categorical vs target: eda_plots/categorical_vs_target/native-country_vs_income.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/56hv4rgj2yn_2z_m3dwvcwhc0000gn/T/ipykernel_19886/4106004475.py:24: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend(title=target_col, loc='center right', bbox_to_anchor=(1.25, 0.5)) # Adjust legend position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logged categorical vs target: eda_plots/categorical_vs_target/income_vs_income.png\n",
      "Finished logging categorical vs target plots.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Categorical vs Target ---\n",
    "print(\"Generating and logging categorical features vs target...\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Ensure 'target_column_name' is defined (e.g., 'income')\n",
    "# target_col = target_column_name\n",
    "target_col = 'income' # Explicitly set if not using variable from ingestion\n",
    "\n",
    "\n",
    "if target_col in df.columns:\n",
    "    mlflow.log_param(\"target_column_for_eda_comparison\", target_col)\n",
    "    for col in categorical_cols: # Reuse categorical_cols\n",
    "        plt.figure(figsize=(12, 6)) # Adjusted size\n",
    "        sns.countplot(\n",
    "            data=df,\n",
    "            y=col, # Again, using y=col often better\n",
    "            hue=target_col, # Color bars based on income level\n",
    "            order=df[col].value_counts().index, # Order categories by frequency\n",
    "            palette=\"Set1\" # Use a different palette for contrast\n",
    "        )\n",
    "        plt.title(f\"{col} vs {target_col}\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Count\", fontsize=12)\n",
    "        plt.ylabel(col, fontsize=12)\n",
    "        plt.legend(title=target_col, loc='center right', bbox_to_anchor=(1.25, 0.5)) # Adjust legend position\n",
    "        plt.tight_layout(rect=[0, 0, 1, 1]) # Adjust layout to potentially make space for legend\n",
    "\n",
    "        # Log the figure directly to MLflow\n",
    "        artifact_path = f\"eda_plots/categorical_vs_target/{col}_vs_{target_col}.png\"\n",
    "        mlflow.log_figure(plt.gcf(), artifact_path)\n",
    "        print(f\"  Logged categorical vs target: {artifact_path}\")\n",
    "\n",
    "        # plt.show() # Optional display\n",
    "        plt.close()\n",
    "else:\n",
    "    print(f\"Target column '{target_col}' not found in DataFrame. Skipping comparison plots.\")\n",
    "    mlflow.log_param(\"target_comparison_skipped\", f\"Column '{target_col}' not found.\")\n",
    "\n",
    "print(\"Finished logging categorical vs target plots.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aea910",
   "metadata": {},
   "source": [
    "# **Phase 3: Data Preprocessing with Scikit-learn Pipelines**\n",
    "\n",
    "After exploring the data, the next logical step is preprocessing. This involves transforming our raw data into a format suitable for machine learning algorithms. Common steps include:\n",
    "\n",
    "1.  **Handling Categorical Features:** Most ML models require numerical input. We need to convert categorical features (like 'workclass', 'education', 'occupation') into a numerical representation, typically using One-Hot Encoding.\n",
    "2.  **Scaling Numerical Features:** Numerical features often have different ranges (e.g., 'age' vs. 'capital-gain'). Scaling them (e.g., using Standardization) ensures that features with larger values don't disproportionately influence the model.\n",
    "3.  **Handling Missing Values:** Although our EDA didn't heavily focus on it, real-world datasets often require strategies for missing data (imputation). *Note: For this example, we'll assume the UCI dataset fetched is relatively clean or that missing value handling was implicitly done during encoding/scaling choices, but in a full project, this would be an explicit step.*\n",
    "\n",
    "**Why Use Scikit-learn Pipelines?**\n",
    "\n",
    "Instead of applying these steps sequentially using pandas or separate Scikit-learn transformers (like in the initial code snippets you provided), we will use Scikit-learn's `Pipeline` and `ColumnTransformer`. This approach offers significant advantages:\n",
    "\n",
    "1.  **Workflow Simplification:** It bundles multiple processing steps into a single object. You call `fit` and `transform` (or `fit_transform`) once on the pipeline, and it handles the sequence internally.\n",
    "2.  **Preventing Data Leakage:** This is crucial. When using tools like `ColumnTransformer`, transformations (like scaling parameters or categories for one-hot encoding) are learned *only* from the training data during the `fit` step. When `transform` is called on the test set (or new data), it uses the *already learned* parameters, preventing information from the test set leaking into the training process. Applying transformations manually *before* splitting the data (as sometimes done with pandas) can lead to overly optimistic results.\n",
    "3.  **Consistency and Reproducibility:** The pipeline ensures the exact same sequence of steps is applied during training, evaluation, and prediction.\n",
    "4.  **Easier Model Persistence:** You can save the entire fitted pipeline (including preprocessing steps and the model) as a single object, simplifying deployment.\n",
    "\n",
    "## **Building the Preprocessing Pipeline**\n",
    "\n",
    "Let's define the preprocessing steps using `ColumnTransformer`, which allows applying different transformations to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06a40405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining preprocessing steps...\n",
      "Identified 6 numerical columns: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Identified 8 categorical columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "\n",
      "Preprocessor defined:\n",
      "ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('num', StandardScaler(),\n",
      "                                 ['age', 'fnlwgt', 'education-num',\n",
      "                                  'capital-gain', 'capital-loss',\n",
      "                                  'hours-per-week']),\n",
      "                                ('cat',\n",
      "                                 OneHotEncoder(drop='first',\n",
      "                                               handle_unknown='ignore',\n",
      "                                               sparse_output=False),\n",
      "                                 ['workclass', 'education', 'marital-status',\n",
      "                                  'occupation', 'relationship', 'race', 'sex',\n",
      "                                  'native-country'])])\n",
      "\n",
      "Preprocessing steps using ColumnTransformer are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Define Preprocessing Pipeline ---\n",
    "# Ensure MLflow tracking URI and credentials are set from the initial setup cell\n",
    "# Assumes 'df_raw' DataFrame and 'target_column_name' ('income') are available.\n",
    "# Assumes 'X_raw' and 'y_raw' (features and target DataFrames from fetch_ucirepo) are available.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression # We'll add a placeholder model later\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "print(\"Defining preprocessing steps...\")\n",
    "\n",
    "# --- 1. Identify Feature Columns ---\n",
    "# We need the names of numerical and categorical columns from the *original* feature set (X_raw)\n",
    "numerical_cols = X_raw.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(numerical_cols)} numerical columns: {numerical_cols}\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Log these columns for reference in MLflow (optional, but good practice)\n",
    "# We can start a specific run for pipeline definition or log later during training.\n",
    "# Let's log them during the first training run that uses this preprocessor.\n",
    "\n",
    "# --- 2. Create the ColumnTransformer ---\n",
    "# This object will apply specific transformers to designated columns.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('name', transformer_object, list_of_columns_to_apply_to)\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), categorical_cols)\n",
    "        # handle_unknown='ignore': Allows the transformer to handle categories seen in test but not train.\n",
    "        # drop='first': Avoids multicollinearity by dropping one category per feature (like pd.get_dummies(drop_first=True)).\n",
    "        # sparse_output=False: Returns a dense numpy array, often easier to work with downstream.\n",
    "    ],\n",
    "    remainder='passthrough' # In case we missed any columns, keep them. Should be empty if cols identified correctly.\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessor defined:\")\n",
    "print(preprocessor)\n",
    "\n",
    "# --- 3. (Placeholder) Define a full pipeline (Preprocessor + Model) ---\n",
    "# We will integrate the actual model later, but let's see the structure.\n",
    "# This pipeline object isn't fitted yet.\n",
    "\n",
    "# Example with Logistic Regression\n",
    "# placeholder_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression(max_iter=1000, random_state=42)) # Example model\n",
    "# ])\n",
    "# print(\"\\nExample full pipeline structure:\")\n",
    "# print(placeholder_pipeline)\n",
    "\n",
    "print(\"\\nPreprocessing steps using ColumnTransformer are ready.\")\n",
    "# We will use the 'preprocessor' object within our main training pipeline later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0316d",
   "metadata": {},
   "source": [
    "### **Explanation:**\n",
    "\n",
    "*   **Column Identification:** We re-identify numerical and categorical columns directly from `X_raw`, the original feature DataFrame. This is important because the pipeline will operate on this raw input.\n",
    "*   **`ColumnTransformer`:**\n",
    "    *   We define two main transformations:\n",
    "        *   `('num', StandardScaler(), numerical_cols)`: Applies `StandardScaler` to all columns listed in `numerical_cols`.\n",
    "        *   `('cat', OneHotEncoder(...), categorical_cols)`: Applies `OneHotEncoder` to all columns in `categorical_cols`. We use `handle_unknown='ignore'` to gracefully handle potential new categories during prediction and `drop='first'` to mimic the behavior of `pd.get_dummies(drop_first=True)`, reducing dimensionality and potential multicollinearity. `sparse_output=False` gives a standard NumPy array.\n",
    "    *   `remainder='passthrough'` ensures any columns not explicitly mentioned are kept as-is (though ideally, all feature columns should be covered by 'num' or 'cat').\n",
    "*   **Placeholder Pipeline:** The commented-out section shows how this `preprocessor` would typically be combined with a classifier (like `LogisticRegression`) into a single `Pipeline` object. This is the structure we'll use for training.\n",
    "\n",
    "# **Phase 4: Model Training and Tracking with Pipelines & Autologging**\n",
    "\n",
    "Now, let's integrate this preprocessing logic into our model training workflow. We'll use the `Pipeline` structure and leverage `mlflow.sklearn.autolog()` for automated tracking.\n",
    "\n",
    "**Benefits of `mlflow.sklearn.autolog()`:**\n",
    "\n",
    "*   **Automatic Logging:** It automatically logs parameters (from the pipeline steps *and* the final estimator), metrics (calculated on a test set if provided or inferred), and the fitted Scikit-learn model/pipeline artifact.\n",
    "*   **Reduced Boilerplate:** Significantly reduces the amount of manual `mlflow.log_param()` and `mlflow.log_metric()` calls needed.\n",
    "*   **Model Signature & Input Example:** Can automatically log the expected input schema (signature) and an example input, which is useful for deployment and validation.\n",
    "*   **Code and Environment:** Often captures details about the execution environment and code versions (depending on configuration).\n",
    "\n",
    "## **Training a Single Model (Logistic Regression) with Pipeline and Autolog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d81e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing target variable and splitting data...\n",
      "Target mapping successful. Value counts:\n",
      "income_binary\n",
      "0    37155\n",
      "1    11687\n",
      "Name: count, dtype: int64\n",
      "Train/Test split complete. Train shape: (39073, 14), Test shape: (9769, 14)\n",
      "\n",
      "Defining the full Scikit-learn pipeline...\n",
      "Pipeline created:\n",
      "\n",
      "Configuring MLflow and enabling autologging...\n",
      "MLflow autologging for Scikit-learn enabled.\n",
      "\n",
      "Starting MLflow run for training...\n",
      "MLflow autologging for Scikit-learn enabled.\n",
      "\n",
      "Starting MLflow run for training...\n",
      "MLflow Run started (ID: 6859ec890605440cba7f4e1a615e17e1, Name: Pipeline_LogisticRegression_Autolog).\n",
      "MLflow Run started (ID: 6859ec890605440cba7f4e1a615e17e1, Name: Pipeline_LogisticRegression_Autolog).\n",
      "Fitting the pipeline...\n",
      "Fitting the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 21:55:59 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 21:56:03 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 21:56:03 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitting complete.\n",
      "Evaluating model on test set (triggers autolog metric calculation)...\n",
      "Pipeline test accuracy score: 0.8519\n",
      "Pipeline test accuracy score: 0.8519\n",
      "MLflow Run Pipeline_LogisticRegression_Autolog finished. Check the MLflow UI for details.\n",
      "MLflow Run Pipeline_LogisticRegression_Autolog finished. Check the MLflow UI for details.\n",
      "üèÉ View run Pipeline_LogisticRegression_Autolog at: http://135.235.251.124/#/experiments/2/runs/6859ec890605440cba7f4e1a615e17e1\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "üèÉ View run Pipeline_LogisticRegression_Autolog at: http://135.235.251.124/#/experiments/2/runs/6859ec890605440cba7f4e1a615e17e1\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "\n",
      "MLflow autologging disabled.\n",
      "\n",
      "MLflow autologging disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run traveling-moose-745 at: http://135.235.251.124/#/experiments/5/runs/a07b500e5829400fb548564a9c81cdcd\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/5\n",
      "üèÉ View run masked-seal-883 at: http://135.235.251.124/#/experiments/7/runs/1483ed422d164045873a9f07a4bd07a6\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/7\n",
      "üèÉ View run legendary-shrimp-629 at: http://135.235.251.124/#/experiments/8/runs/be3aa64addbe460ea4da660b2df751e5\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Train Single Model with Pipeline & Autolog ---\n",
    "# Make sure MLflow environment variables and tracking URI are set.\n",
    "# Assumes 'X_raw', 'y_raw', 'preprocessor' are defined from previous cells.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report # For potential manual checks\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import numpy as np # For potential type checks\n",
    "\n",
    "# --- 1. Prepare Data (Map Target and Split) ---\n",
    "# Ensure the target variable is binary (0/1) BEFORE splitting.\n",
    "# Handle potential variations like '.' suffixes found in raw data.\n",
    "print(\"Preparing target variable and splitting data...\")\n",
    "try:\n",
    "    # Clean potential variations and map to 0/1\n",
    "    y_mapped = y_raw['income'].str.strip().replace({'<=50K.': '<=50K', '>50K.': '>50K'})\n",
    "    y_binary = y_mapped.apply(lambda x: 1 if x == '>50K' else 0).astype(np.int32) # Ensure integer type\n",
    "\n",
    "    target_name = 'income_binary' # Define a clear name for the processed target\n",
    "    y_binary.name = target_name\n",
    "\n",
    "    print(f\"Target mapping successful. Value counts:\\n{y_binary.value_counts()}\")\n",
    "\n",
    "    # Split the *raw* features and the *binary* target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_raw, y_binary,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_binary # Stratify helps maintain class proportion in splits\n",
    "    )\n",
    "    print(f\"Train/Test split complete. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during target mapping or data split: {e}\")\n",
    "    # Consider stopping execution if data prep fails\n",
    "    raise\n",
    "\n",
    "# --- 2. Define the Full Pipeline ---\n",
    "# Combine the preprocessor with the chosen model.\n",
    "print(\"\\nDefining the full Scikit-learn pipeline...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear') # Using liblinear for potential L1 later\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # The ColumnTransformer defined earlier\n",
    "    ('classifier', lr_model)        # The Logistic Regression model\n",
    "])\n",
    "print(\"Pipeline created:\")\n",
    "\n",
    "# --- 3. Configure MLflow and Enable Autologging ---\n",
    "print(\"\\nConfiguring MLflow and enabling autologging...\")\n",
    "mlflow.set_experiment(\"UCI Adult Income Prediction - Centralized\") # Ensure consistent experiment name\n",
    "\n",
    "# Enable autologging\n",
    "# - log_input_examples=True: Records a sample of training data.\n",
    "# - log_model_signatures=True: Infers the model input/output schema.\n",
    "# - registered_model_name: Optionally register the logged model in MLflow Model Registry.\n",
    "mlflow.sklearn.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True,\n",
    "    # registered_model_name=\"AdultIncomeLogisticRegressionPipeline\", # Uncomment to register\n",
    "    disable=False # Explicitly enable (default)\n",
    ")\n",
    "print(\"MLflow autologging for Scikit-learn enabled.\")\n",
    "\n",
    "# --- 4. Train the Model within an MLflow Run ---\n",
    "print(\"\\nStarting MLflow run for training...\")\n",
    "# Autologging works within the context of an active MLflow run.\n",
    "run_name = \"Pipeline_LogisticRegression_Autolog\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run started (ID: {run_id}, Name: {run_name}).\")\n",
    "\n",
    "    # Log the columns used by the preprocessor manually for clarity\n",
    "    mlflow.log_param(\"numerical_features\", numerical_cols)\n",
    "    mlflow.log_param(\"categorical_features\", categorical_cols)\n",
    "    mlflow.log_param(\"target_variable_processed\", target_name)\n",
    "\n",
    "    # Fit the *entire* pipeline on the training data\n",
    "    print(\"Fitting the pipeline...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(\"Pipeline fitting complete.\")\n",
    "\n",
    "    # Autologging automatically handles:\n",
    "    # - Logging pipeline parameters (including preprocessor steps and classifier params).\n",
    "    # - Training the model.\n",
    "    # - Evaluating the model on the test set (if X_test, y_test are available implicitly via fit).\n",
    "    #   Note: Autolog usually infers evaluation based on typical fit/predict patterns,\n",
    "    #         or sometimes requires explicit scoring call depending on version/context.\n",
    "    #         Let's explicitly evaluate to ensure metrics are logged by autolog.\n",
    "    print(\"Evaluating model on test set (triggers autolog metric calculation)...\")\n",
    "    test_score = pipeline.score(X_test, y_test) # .score often triggers autolog metrics\n",
    "    print(f\"Pipeline test accuracy score: {test_score:.4f}\")\n",
    "\n",
    "    # - Logging metrics (accuracy, F1, etc. - might vary by MLflow version).\n",
    "    # - Logging the fitted pipeline artifact (including preprocessor and model).\n",
    "    # - Logging input example and model signature.\n",
    "\n",
    "    # (Optional) Add Custom Tags or Artifacts if needed\n",
    "    mlflow.set_tag(\"model_variant\", \"LogisticRegression\")\n",
    "    mlflow.set_tag(\"pipeline_description\", \"StandardScaler + OHE(drop_first)\")\n",
    "    # Example: Log classification report manually if autolog doesn't capture it as desired\n",
    "    # y_pred_test = pipeline.predict(X_test)\n",
    "    # report = classification_report(y_test, y_pred_test)\n",
    "    # mlflow.log_text(report, \"classification_report_test.txt\")\n",
    "\n",
    "    print(f\"MLflow Run {run_name} finished. Check the MLflow UI for details.\")\n",
    "\n",
    "# --- 5. Disable Autologging (Good Practice) ---\n",
    "# Disable if you plan to run other non-Scikit-learn code afterwards\n",
    "# that you don't want autologged, or if setting up a new autolog config.\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "print(\"\\nMLflow autologging disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363bcd1",
   "metadata": {},
   "source": [
    "### **Explanation:**\n",
    "\n",
    "1.  **Data Preparation:** We perform the critical step of mapping the target variable (`income`) to a binary format (0/1) *before* the `train_test_split`. This ensures the split happens on the final target representation. We use `stratify=y_binary` to maintain the class balance between train and test sets, which is important for classification tasks.\n",
    "2.  **Full Pipeline Definition:** We create the `Pipeline` object, explicitly listing the steps: first our `preprocessor` (`ColumnTransformer`), then the `classifier` (`LogisticRegression`).\n",
    "3.  **Autologging Setup:** We set the MLflow experiment name and enable `mlflow.sklearn.autolog()`. We include `log_input_examples` and `log_model_signatures` for richer tracking. Optionally, `registered_model_name` can be set to directly register the resulting model in the MLflow Model Registry upon logging.\n",
    "4.  **Training within MLflow Run:**\n",
    "    *   We start an MLflow run using `with mlflow.start_run(...)`.\n",
    "    *   We manually log the feature lists used by the preprocessor for extra clarity using `mlflow.log_param`.\n",
    "    *   The core step is `pipeline.fit(X_train, y_train)`. This single command trains the entire pipeline: the preprocessor learns scaling parameters and categories from `X_train`, transforms `X_train`, and then trains the `LogisticRegression` model on the transformed data.\n",
    "    *   We call `pipeline.score(X_test, y_test)`. For many autologging setups, performing an evaluation step like `.score()` or `.predict()` after `.fit()` helps ensure that test metrics are calculated and logged automatically.\n",
    "    *   Autologging captures the pipeline's parameters (e.g., `C` from Logistic Regression, `handle_unknown` from OHE), evaluation metrics (accuracy, F1, etc., calculated on the test set implicitly or via the `.score` call), and the fitted pipeline itself as an artifact.\n",
    "    *   We add optional custom tags using `mlflow.set_tag` for easier filtering/grouping in the UI.\n",
    "5.  **Disable Autologging:** It's good practice to disable autologging when you're done with the specific training block to avoid unintended logging from subsequent code.\n",
    "\n",
    "### **Viewing Autologged Results in MLflow UI**\n",
    "\n",
    "After running the cell above:\n",
    "\n",
    "1.  Go back to your MLflow UI (http://\\<YOUR\\_EXTERNAL\\_IP\\>).\n",
    "2.  Navigate to the \"UCI Adult Income Prediction - Centralized\" experiment.\n",
    "3.  Find the run named \"Pipeline\\_LogisticRegression\\_Autolog\".\n",
    "4.  **Parameters:** You should see parameters logged from *both* the `preprocessor` (like `remainder`, `cat__handle_unknown`) and the `classifier` (like `classifier__C`, `classifier__max_iter`). MLflow prefixes parameters from pipeline steps with the step name (e.g., `classifier__`). You'll also see the manually logged `numerical_features` and `categorical_features`.\n",
    "5.  **Metrics:** Autologging should have captured metrics like `test_accuracy_score`, `test_f1_score`, `test_precision_score`, `test_recall_score`, etc. (The exact names might vary slightly based on MLflow/Scikit-learn versions). These are calculated using `X_test` and `y_test`.\n",
    "6.  **Artifacts:**\n",
    "    *   You'll find a `model` directory containing the *entire fitted pipeline* saved in MLflow's `python_function` flavor (and potentially the Scikit-learn flavor). This artifact includes the fitted preprocessor and the trained Logistic Regression model.\n",
    "    *   You might also see `input_example.json` and `model_signature.json` (if `log_input_examples` and `log_model_signatures` were successful).\n",
    "    *   Any manually logged artifacts (like the classification report if you uncommented that part) would also appear here.\n",
    "\n",
    "\n",
    "# **Phase 5: Training Multiple Models with Pipelines and Autologging**\n",
    "\n",
    "This demonstrates how easily you can swap out model estimators within the same preprocessing framework and train multiple models with different hyperparameters using defined model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "499dc1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up for multiple model training with pipelines...\n",
      "Defined 7 model configurations.\n",
      "MLflow autologging enabled for the training loop.\n",
      "Logging runs to experiment: 'UCI Adult Income Prediction - Centralized'\n",
      "\n",
      "--- Training LogisticRegression_C_1_0 ---\n",
      "Fitting pipeline for LogisticRegression_C_1_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:29:29 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 18:29:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogisticRegression_C_1_0 Test Accuracy: 0.8519\n",
      "üèÉ View run Pipeline_LogisticRegression_C_1_0_Autolog at: http://135.235.251.124/#/experiments/2/runs/c9f12d5d3ff740b0b145a5e83e239ac7\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished LogisticRegression_C_1_0 ---\n",
      "\n",
      "--- Training RandomForest_n100_md10 ---\n",
      "Fitting pipeline for RandomForest_n100_md10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:29:42 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 18:29:46 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_n100_md10 Test Accuracy: 0.8597\n",
      "üèÉ View run Pipeline_RandomForest_n100_md10_Autolog at: http://135.235.251.124/#/experiments/2/runs/747dd40a876f4addbbddc13f64703e4c\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished RandomForest_n100_md10 ---\n",
      "\n",
      "--- Training RandomForest_n200_md15 ---\n",
      "Fitting pipeline for RandomForest_n200_md15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:29:56 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 18:30:00 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_n200_md15 Test Accuracy: 0.8654\n",
      "üèÉ View run Pipeline_RandomForest_n200_md15_Autolog at: http://135.235.251.124/#/experiments/2/runs/5964418eb79e465dae3cadeb7edb1c38\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished RandomForest_n200_md15 ---\n",
      "\n",
      "--- Training XGBoost_default ---\n",
      "Fitting pipeline for XGBoost_default...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:30:13 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:30:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/05/12 18:30:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_default Test Accuracy: 0.8759\n",
      "üèÉ View run Pipeline_XGBoost_default_Autolog at: http://135.235.251.124/#/experiments/2/runs/65f88853841548f1a042b5a60688a3de\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished XGBoost_default ---\n",
      "\n",
      "--- Training XGBoost_lr01_md3 ---\n",
      "Fitting pipeline for XGBoost_lr01_md3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:30:26 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [18:30:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/05/12 18:30:29 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_lr01_md3 Test Accuracy: 0.8700\n",
      "üèÉ View run Pipeline_XGBoost_lr01_md3_Autolog at: http://135.235.251.124/#/experiments/2/runs/2f6eee9cba6142ddb7506ea81c19524f\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished XGBoost_lr01_md3 ---\n",
      "\n",
      "--- Training KNN_n5_minkowski ---\n",
      "Fitting pipeline for KNN_n5_minkowski...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:30:38 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 18:30:48 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  KNN_n5_minkowski Test Accuracy: 0.8314\n",
      "üèÉ View run Pipeline_KNN_n5_minkowski_Autolog at: http://135.235.251.124/#/experiments/2/runs/bf4192c152364996adaa0edfcecd20b5\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished KNN_n5_minkowski ---\n",
      "\n",
      "--- Training GradientBoosting_n100_lr01_md3 ---\n",
      "Fitting pipeline for GradientBoosting_n100_lr01_md3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 18:31:00 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/05/12 18:31:11 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_n100_lr01_md3 Test Accuracy: 0.8715\n",
      "üèÉ View run Pipeline_GradientBoosting_n100_lr01_md3_Autolog at: http://135.235.251.124/#/experiments/2/runs/37974f7eaf4245179a0dca3959b7c01f\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "--- Finished GradientBoosting_n100_lr01_md3 ---\n",
      "\n",
      "Training loop complete. MLflow autologging disabled.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Train Multiple Models with Pipelines & Autolog ---\n",
    "# Ensure MLflow environment variables and tracking URI are set.\n",
    "# Assumes 'X_raw', 'y_raw', 'X_train', 'X_test', 'y_train', 'y_test' are defined.\n",
    "# Assumes 'numerical_cols', 'categorical_cols' are identified based on X_raw.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC # Note: SVC can be slow without tuning/sampling\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Add other necessary imports: Pipeline, ColumnTransformer, StandardScaler, OneHotEncoder, mlflow, os, etc.\n",
    "\n",
    "print(\"Setting up for multiple model training with pipelines...\")\n",
    "\n",
    "# --- 1. Define Preprocessor (if not already defined globally) ---\n",
    "# Ensure the preprocessor using numerical_cols and categorical_cols from X_raw is available.\n",
    "# (Assuming 'preprocessor' ColumnTransformer object exists from previous cells)\n",
    "if 'preprocessor' not in locals():\n",
    "     print(\"Re-defining preprocessor...\")\n",
    "     preprocessor = ColumnTransformer(\n",
    "         transformers=[\n",
    "             ('num', StandardScaler(), numerical_cols),\n",
    "             ('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), categorical_cols)\n",
    "         ],\n",
    "         remainder='passthrough'\n",
    "     )\n",
    "     print(\"Preprocessor re-defined.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Models Configuration ---\n",
    "# Use a dictionary for clarity, key is descriptive name, value is model instance.\n",
    "models_config = {\n",
    "    \"LogisticRegression_C_1_0\": LogisticRegression(max_iter=2000, solver=\"liblinear\", C=1.0, random_state=42),\n",
    "    \"RandomForest_n100_md10\": RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    \"RandomForest_n200_md15\": RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost_default\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42, n_jobs=-1),\n",
    "    \"XGBoost_lr01_md3\": XGBClassifier(learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric=\"logloss\", random_state=42, n_jobs=-1),\n",
    "    \"KNN_n5_minkowski\": KNeighborsClassifier(n_neighbors=5, metric='minkowski', n_jobs=-1),\n",
    "    \"GradientBoosting_n100_lr01_md3\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    # Add more models or variations as needed\n",
    "    # \"SVC_linear\": SVC(kernel='linear', probability=True, random_state=42), # Can be slow\n",
    "}\n",
    "print(f\"Defined {len(models_config)} model configurations.\")\n",
    "\n",
    "# --- 3. Enable Autologging for the Loop ---\n",
    "# Make sure it's enabled before the loop starts.\n",
    "# Set registered_model_name=None if you don't want to register every single variant automatically.\n",
    "mlflow.sklearn.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True,\n",
    "    registered_model_name=None, # Avoid registering every model in the loop automatically\n",
    "    disable=False # Ensure it's enabled\n",
    ")\n",
    "print(\"MLflow autologging enabled for the training loop.\")\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "experiment_name = \"UCI Adult Income Prediction - Centralized\" # Or a new one like \"Adult_Pipeline_Comparison\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"Logging runs to experiment: '{experiment_name}'\")\n",
    "\n",
    "for model_name, model_instance in models_config.items():\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "\n",
    "    # Create the full pipeline for *this specific model*\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), # Reuse the same preprocessor\n",
    "        ('classifier', model_instance)  # Insert the current model\n",
    "    ])\n",
    "\n",
    "    # Start a unique run for this model pipeline\n",
    "    run_name = f\"Pipeline_{model_name}_Autolog\"\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        mlflow.set_tag(\"model_name\", model_name) # Tag with the specific model name\n",
    "        mlflow.set_tag(\"pipeline_used\", \"standard_prep_v1\") # Tag the preprocessing version\n",
    "\n",
    "        # Log feature lists for reference (can be repetitive but ensures it's in each run)\n",
    "        mlflow.log_param(\"numerical_features\", numerical_cols)\n",
    "        mlflow.log_param(\"categorical_features\", categorical_cols)\n",
    "\n",
    "        print(f\"Fitting pipeline for {model_name}...\")\n",
    "        try:\n",
    "            # Fit the pipeline\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate (helps ensure autolog captures test metrics)\n",
    "            test_score = pipeline.score(X_test, y_test)\n",
    "            print(f\"  {model_name} Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "            # Autologging handles params, metrics, model artifact for this pipeline\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR training {model_name}: {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed\")\n",
    "            mlflow.log_param(\"error_message\", str(e))\n",
    "            # Optionally log stack trace or more details\n",
    "\n",
    "    print(f\"--- Finished {model_name} ---\")\n",
    "\n",
    "# --- 5. Disable Autologging After Loop ---\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "print(\"\\nTraining loop complete. MLflow autologging disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef05bc",
   "metadata": {},
   "source": [
    "- Preprocessor Reuse: The preprocessor (ColumnTransformer) is defined once (or ensured to exist) outside the loop.\n",
    "- Pipeline Creation Inside Loop: For each model_instance from models_config, a new Pipeline is created, combining the standard preprocessor with the specific model_instance.\n",
    "- Unique Run Per Model: Each iteration of the loop starts a new MLflow run with a descriptive name (e.g., \"Pipeline\\_RandomForest\\_n100\\_md10\\_Autolog\").\n",
    "- Autologging Per Run: autolog() captures the parameters, metrics, and the specific fitted pipeline (e.g., preprocessor + RandomForest) for each run.\n",
    "- Tagging: We add tags (model_name, pipeline_used) to each run for easier filtering and comparison in the MLflow UI.\n",
    "- Error Handling: Basic try‚Ä¶except block added to catch errors during fitting specific models without stopping the entire loop, logging an error message and status tag to MLflow.\n",
    "- Configuration: models_config dictionary makes adding/removing models easy. n_jobs=-1 is added to classifiers where applicable to speed up training using multiple CPU cores. use_label_encoder=False added for XGBClassifier.\n",
    "\n",
    "# **Phase 6: Integrating TensorFlow and Deep Learning with MLflow Autologging**\n",
    "\n",
    "Beyond Scikit-learn, MLflow offers robust support for popular deep learning frameworks like TensorFlow and PyTorch. This is crucial, as many complex problems benefit from the power of neural networks. MLflow's autologging for TensorFlow (`mlflow.tensorflow.autolog()`) simplifies the tracking of deep learning experiments significantly, capturing essential information with minimal code changes.\n",
    "\n",
    "### **The Power of `mlflow.tensorflow.autolog()`**\n",
    "\n",
    "When enabled, `mlflow.tensorflow.autolog()` automatically logs a wealth of information during your Keras/TensorFlow model training, including:\n",
    "\n",
    "1.  **Model Parameters:** Hyperparameters like learning rate, batch size, number of epochs, and optimizer configurations.\n",
    "2.  **Model Summary:** The architecture of your neural network.\n",
    "3.  **Training Metrics:** Metrics specified in `model.compile()` (e.g., loss, accuracy) are logged for each epoch, for both training and validation sets. This allows you to visualize learning curves directly in the MLflow UI.\n",
    "4.  **Callbacks:** Information about Keras callbacks used, such as `EarlyStopping` parameters.\n",
    "5.  **Fitted Model:** The trained TensorFlow/Keras model is saved as an MLflow artifact, typically in TensorFlow's SavedModel format, making it ready for deployment.\n",
    "6.  **TensorBoard Logs:** If you're using TensorBoard, autologging can also capture the TensorBoard log directory.\n",
    "7.  **(Optional) Model Signature & Input Example:** Similar to Scikit-learn, it can infer and log the model's input/output schema and an example input.\n",
    "\n",
    "This comprehensive logging helps in understanding model behavior, comparing different architectures or hyperparameters, and reproducing results.\n",
    "\n",
    "### **Practical Example: Training a Deep Neural Network**\n",
    "\n",
    "Let's train a simple Deep Neural Network (DNN) for our UCI Adult Income prediction task using TensorFlow (Keras API) and see how `mlflow.tensorflow.autolog()` works.\n",
    "\n",
    "For this example, we will use the preprocessed data (`X_train`, `X_test`, `y_train`, `y_test`) that was saved earlier (after one-hot encoding and scaling were applied directly). In a production scenario with pipelines, you would typically extract the transformed numerical arrays from your Scikit-learn pipeline to feed into TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow and loading preprocessed data...\n",
      "Data loaded. X_train shape: (39073, 102), y_train shape: (39073,)\n",
      "Target arrays reshaped. y_train shape: (39073,), y_test shape: (9769,)\n",
      "\n",
      "Enabling MLflow autologging for TensorFlow...\n",
      "TensorFlow autologging enabled.\n",
      "\n",
      "Starting MLflow run: Deep_Neural_Network_TensorFlow_Autolog\n",
      "MLflow Run started (ID: 743e45a2945f4711910f9896afcfdf70).\n",
      "Building the Keras Sequential model...\n",
      "Model built successfully.\n",
      "Compiling the model...\n",
      "Model compiled.\n",
      "EarlyStopping callback configured: monitor='val_loss', patience=10.\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 20:03:43 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'pandas.core.frame.DataFrame'>. Dataset logging skipped.\n",
      "2025/05/12 20:03:43 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'DataFrame' object has no attribute 'flatten'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m586/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8336 - loss: 0.3489 - pr_auc: 0.4842 - roc_auc: 0.8184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8354 - loss: 0.3457 - pr_auc: 0.4906 - roc_auc: 0.8219 - val_accuracy: 0.8999 - val_loss: 0.2195 - val_pr_auc: 0.7547 - val_roc_auc: 0.9419\n",
      "Epoch 2/100\n",
      "\u001b[1m599/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8968 - loss: 0.2252 - pr_auc: 0.7339 - roc_auc: 0.9366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8969 - loss: 0.2251 - pr_auc: 0.7341 - roc_auc: 0.9366 - val_accuracy: 0.9030 - val_loss: 0.2121 - val_pr_auc: 0.7690 - val_roc_auc: 0.9445\n",
      "Epoch 3/100\n",
      "\u001b[1m593/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8995 - loss: 0.2148 - pr_auc: 0.7639 - roc_auc: 0.9415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8996 - loss: 0.2148 - pr_auc: 0.7639 - roc_auc: 0.9415 - val_accuracy: 0.9011 - val_loss: 0.2092 - val_pr_auc: 0.7735 - val_roc_auc: 0.9459\n",
      "Epoch 4/100\n",
      "\u001b[1m601/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9019 - loss: 0.2115 - pr_auc: 0.7688 - roc_auc: 0.9438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9019 - loss: 0.2115 - pr_auc: 0.7689 - roc_auc: 0.9439 - val_accuracy: 0.9041 - val_loss: 0.2088 - val_pr_auc: 0.7779 - val_roc_auc: 0.9456\n",
      "Epoch 5/100\n",
      "\u001b[1m578/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9042 - loss: 0.2060 - pr_auc: 0.7828 - roc_auc: 0.9466"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9042 - loss: 0.2060 - pr_auc: 0.7827 - roc_auc: 0.9466 - val_accuracy: 0.9060 - val_loss: 0.2076 - val_pr_auc: 0.7793 - val_roc_auc: 0.9464\n",
      "Epoch 6/100\n",
      "\u001b[1m607/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9031 - loss: 0.2064 - pr_auc: 0.7714 - roc_auc: 0.9463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9031 - loss: 0.2064 - pr_auc: 0.7715 - roc_auc: 0.9463 - val_accuracy: 0.9058 - val_loss: 0.2073 - val_pr_auc: 0.7799 - val_roc_auc: 0.9465\n",
      "Epoch 7/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.2011 - pr_auc: 0.7966 - roc_auc: 0.9496 - val_accuracy: 0.9037 - val_loss: 0.2077 - val_pr_auc: 0.7784 - val_roc_auc: 0.9462\n",
      "Epoch 8/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9060 - loss: 0.2015 - pr_auc: 0.7953 - roc_auc: 0.9503 - val_accuracy: 0.9044 - val_loss: 0.2095 - val_pr_auc: 0.7791 - val_roc_auc: 0.9451\n",
      "Epoch 9/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9089 - loss: 0.1961 - pr_auc: 0.7932 - roc_auc: 0.9513 - val_accuracy: 0.9043 - val_loss: 0.2086 - val_pr_auc: 0.7771 - val_roc_auc: 0.9455\n",
      "Epoch 10/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9101 - loss: 0.1935 - pr_auc: 0.7992 - roc_auc: 0.9538 - val_accuracy: 0.9052 - val_loss: 0.2092 - val_pr_auc: 0.7757 - val_roc_auc: 0.9456\n",
      "Epoch 11/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9097 - loss: 0.1954 - pr_auc: 0.8090 - roc_auc: 0.9532 - val_accuracy: 0.9051 - val_loss: 0.2101 - val_pr_auc: 0.7751 - val_roc_auc: 0.9449\n",
      "Epoch 12/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9130 - loss: 0.1888 - pr_auc: 0.8142 - roc_auc: 0.9557 - val_accuracy: 0.9047 - val_loss: 0.2114 - val_pr_auc: 0.7740 - val_roc_auc: 0.9448\n",
      "Epoch 13/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9074 - loss: 0.1951 - pr_auc: 0.8094 - roc_auc: 0.9531 - val_accuracy: 0.9043 - val_loss: 0.2102 - val_pr_auc: 0.7766 - val_roc_auc: 0.9449\n",
      "Epoch 14/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9142 - loss: 0.1881 - pr_auc: 0.8174 - roc_auc: 0.9562 - val_accuracy: 0.9039 - val_loss: 0.2127 - val_pr_auc: 0.7733 - val_roc_auc: 0.9436\n",
      "Epoch 15/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9110 - loss: 0.1910 - pr_auc: 0.8115 - roc_auc: 0.9541 - val_accuracy: 0.9040 - val_loss: 0.2127 - val_pr_auc: 0.7731 - val_roc_auc: 0.9432\n",
      "Epoch 16/100\n",
      "\u001b[1m611/611\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9148 - loss: 0.1851 - pr_auc: 0.8250 - roc_auc: 0.9578 - val_accuracy: 0.9029 - val_loss: 0.2157 - val_pr_auc: 0.7696 - val_roc_auc: 0.9427\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 20:04:09 WARNING mlflow.tensorflow: Failed to gather input example: Cannot log input example or model signature for input with type <class 'pandas.core.frame.DataFrame'>. TensorFlow Keras autologging can only log input examples and model signatures for the following input types: numpy.ndarray, dict[string -> numpy.ndarray], tensorflow.keras.utils.Sequence, and tensorflow.data.Dataset (TensorFlow >= 2.1.0 required)\n",
      "2025/05/12 20:04:09 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: Cannot log input example or model signature for input with type <class 'pandas.core.frame.DataFrame'>. TensorFlow Keras autologging can only log input examples and model signatures for the following input types: numpy.ndarray, dict[string -> numpy.ndarray], tensorflow.keras.utils.Sequence, and tensorflow.data.Dataset (TensorFlow >= 2.1.0 required)\n",
      "2025/05/12 20:04:09 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "\u001b[31m2025/05/12 20:04:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "Performing final predictions and logging detailed evaluation metrics...\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step\n",
      "Final evaluation metrics (accuracy, precision, recall, F1, AUCs) logged manually.\n",
      "Confusion matrix plots logged as artifacts.\n",
      "Classification reports logged as artifacts.\n",
      "Learning curves plot logged as artifact.\n",
      "MLflow Run Deep_Neural_Network_TensorFlow_Autolog (ID: 743e45a2945f4711910f9896afcfdf70) finished successfully.\n",
      "üèÉ View run Deep_Neural_Network_TensorFlow_Autolog at: http://135.235.251.124/#/experiments/2/runs/743e45a2945f4711910f9896afcfdf70\n",
      "üß™ View experiment at: http://135.235.251.124/#/experiments/2\n",
      "\n",
      "MLflow autologging for TensorFlow disabled.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: TensorFlow DNN Training with Autolog ---\n",
    "import os\n",
    "import joblib # To load our preprocessed data\n",
    "import mlflow\n",
    "import mlflow.tensorflow # Specific TensorFlow integration\n",
    "from mlflow.models.signature import infer_signature # For model schema\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping # Useful callback\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    log_loss, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# --- 1. Setup and Load Data ---\n",
    "print(\"Setting up MLflow and loading preprocessed data...\")\n",
    "load_dotenv() # Load environment variables (MLFLOW_TRACKING_URI, etc.)\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "mlflow.set_experiment(\"UCI Adult Income Prediction - Centralized\") # Consistent experiment\n",
    "\n",
    "# Load the preprocessed data that was saved earlier\n",
    "# This data has already been one-hot encoded and scaled.\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = joblib.load(\"../data/train_test_data.pkl\")\n",
    "    print(f\"Data loaded. X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: train_test_data.pkl not found. Please ensure preprocessing steps were run and data was saved.\")\n",
    "    # Depending on notebook flow, you might want to raise an error or stop\n",
    "    raise\n",
    "\n",
    "# Ensure y_train and y_test are 1D arrays, as Keras expects for binary classification\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "y_test = np.asarray(y_test).ravel()\n",
    "print(f\"Target arrays reshaped. y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# --- 2. Enable MLflow Autologging for TensorFlow ---\n",
    "print(\"\\nEnabling MLflow autologging for TensorFlow...\")\n",
    "# Key options:\n",
    "# - log_models: (default True) Save the trained Keras model.\n",
    "# - log_every_n_epoch: Log metrics every N epochs (default 1).\n",
    "# - registered_model_name: Optionally register the model directly.\n",
    "mlflow.tensorflow.autolog(\n",
    "    log_model_signatures=True, # Log input/output schema\n",
    "    log_input_examples=True,   # Log an input example\n",
    "    # registered_model_name=\"AdultIncomeDNN\", # Uncomment to register model\n",
    "    disable=False # Ensure it's enabled\n",
    ")\n",
    "print(\"TensorFlow autologging enabled.\")\n",
    "\n",
    "# --- 3. Train the TensorFlow Model within an MLflow Run ---\n",
    "run_name = \"Deep_Neural_Network_TensorFlow_Autolog\"\n",
    "print(f\"\\nStarting MLflow run: {run_name}\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run started (ID: {run_id}).\")\n",
    "\n",
    "    # --- A. Build the DNN Model ---\n",
    "    print(\"Building the Keras Sequential model...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],), name=\"input_layer\"), # Input layer matching feature dimension\n",
    "        tf.keras.layers.Dense(128, activation='relu', name=\"dense_1\"),\n",
    "        tf.keras.layers.Dropout(0.3, name=\"dropout_1\"),\n",
    "        tf.keras.layers.Dense(64, activation='relu', name=\"dense_2\"),\n",
    "        tf.keras.layers.Dropout(0.3, name=\"dropout_2\"),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', name=\"output_layer\") # Output layer for binary classification\n",
    "    ])\n",
    "    print(\"Model built successfully.\")\n",
    "    # Autolog will capture model.summary()\n",
    "\n",
    "    # --- B. Compile the Model ---\n",
    "    print(\"Compiling the model...\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Autolog captures optimizer_name, learning_rate\n",
    "        loss='binary_crossentropy',      # Autolog captures loss function\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='roc_auc'), tf.keras.metrics.AUC(name='pr_auc', curve='PR')] # Autolog captures these metrics per epoch\n",
    "    )\n",
    "    print(\"Model compiled.\")\n",
    "\n",
    "    # --- C. Define EarlyStopping Callback ---\n",
    "    # Autologging will also log parameters of callbacks like EarlyStopping.\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',       # Metric to monitor\n",
    "        patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=1,\n",
    "        restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    "    )\n",
    "    print(f\"EarlyStopping callback configured: monitor='val_loss', patience={early_stop.patience}.\")\n",
    "\n",
    "    # --- D. Train the Model ---\n",
    "    print(\"Training the model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test), # Autolog uses this for validation metrics\n",
    "        epochs=100,                       # Autolog captures epochs\n",
    "        batch_size=64,                    # Autolog captures batch_size\n",
    "        callbacks=[early_stop],           # Autolog logs callback info\n",
    "        verbose=1                         # Set to 1 or 2 to see Keras progress\n",
    "    )\n",
    "    print(\"Model training complete.\")\n",
    "    # Autologging automatically logs epoch-wise metrics (loss, acc, val_loss, val_acc, etc.)\n",
    "    # and the final trained model artifact.\n",
    "\n",
    "    # --- E. (Optional) Manual Logging for Detailed Final Evaluation ---\n",
    "    # While autolog captures epoch-wise metrics and the model, we might want specific\n",
    "    # overall performance metrics or visualizations not covered by default.\n",
    "    print(\"\\nPerforming final predictions and logging detailed evaluation metrics...\")\n",
    "    y_train_prob_tf = model.predict(X_train).ravel()\n",
    "    y_test_prob_tf = model.predict(X_test).ravel()\n",
    "    y_train_pred_tf = (y_train_prob_tf >= 0.5).astype(int)\n",
    "    y_test_pred_tf = (y_test_prob_tf >= 0.5).astype(int)\n",
    "\n",
    "    # Helper function for logging common classification metrics\n",
    "    def log_classification_metrics_manual(y_true, y_pred, y_prob, prefix):\n",
    "        mlflow.log_metric(f\"{prefix}_accuracy_final\", accuracy_score(y_true, y_pred))\n",
    "        mlflow.log_metric(f\"{prefix}_precision_final\", precision_score(y_true, y_pred))\n",
    "        mlflow.log_metric(f\"{prefix}_recall_final\", recall_score(y_true, y_pred))\n",
    "        mlflow.log_metric(f\"{prefix}_f1_score_final\", f1_score(y_true, y_pred))\n",
    "        mlflow.log_metric(f\"{prefix}_log_loss_final\", log_loss(y_true, y_prob))\n",
    "        mlflow.log_metric(f\"{prefix}_roc_auc_final\", roc_auc_score(y_true, y_prob)) # This will be overall ROC AUC\n",
    "        mlflow.log_metric(f\"{prefix}_pr_auc_final\", average_precision_score(y_true, y_prob)) # Overall PR AUC\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        mlflow.log_metric(f\"{prefix}_true_negative_final\", tn)\n",
    "        mlflow.log_metric(f\"{prefix}_false_positive_final\", fp)\n",
    "        mlflow.log_metric(f\"{prefix}_false_negative_final\", fn)\n",
    "        mlflow.log_metric(f\"{prefix}_true_positive_final\", tp)\n",
    "\n",
    "    # Log final metrics for train and test sets\n",
    "    log_classification_metrics_manual(y_train, y_train_pred_tf, y_train_prob_tf, \"train\")\n",
    "    log_classification_metrics_manual(y_test, y_test_pred_tf, y_test_prob_tf, \"test\")\n",
    "    print(\"Final evaluation metrics (accuracy, precision, recall, F1, AUCs) logged manually.\")\n",
    "\n",
    "    # Log confusion matrix plots manually\n",
    "    for prefix_cm, y_true_cm, y_pred_cm in [\n",
    "        (\"Train_Final\", y_train, y_train_pred_tf),\n",
    "        (\"Test_Final\", y_test, y_test_pred_tf)\n",
    "    ]:\n",
    "        cm = confusion_matrix(y_true_cm, y_pred_cm)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix - {prefix_cm}\")\n",
    "        # Save to a temporary file to log as an artifact\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
    "            plt.savefig(tmpfile.name)\n",
    "            mlflow.log_artifact(tmpfile.name, artifact_path=\"evaluation_plots/confusion_matrices\")\n",
    "        plt.close() # Close the plot to free memory\n",
    "    print(\"Confusion matrix plots logged as artifacts.\")\n",
    "\n",
    "    # Log classification reports manually\n",
    "    for prefix_cr, y_true_cr, y_pred_cr in [\n",
    "        (\"train_final\", y_train, y_train_pred_tf),\n",
    "        (\"test_final\", y_test, y_test_pred_tf)\n",
    "    ]:\n",
    "        report = classification_report(y_true_cr, y_pred_cr)\n",
    "        # Save to a temporary file to log as an artifact\n",
    "        with tempfile.NamedTemporaryFile(\"w+\", delete=False, suffix=\".txt\") as tmpfile:\n",
    "            tmpfile.write(f\"Classification Report - {prefix_cr.capitalize()}\\n\\n\")\n",
    "            tmpfile.write(report)\n",
    "            tmpfile.flush() # Ensure content is written to disk\n",
    "            mlflow.log_artifact(tmpfile.name, artifact_path=\"evaluation_reports/classification_reports\")\n",
    "    print(\"Classification reports logged as artifacts.\")\n",
    "\n",
    "    # (Optional) Log learning curves plot\n",
    "    if history:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if 'accuracy' in history.history and 'val_accuracy' in history.history:\n",
    "            plt.plot(history.history['accuracy'])\n",
    "            plt.plot(history.history['val_accuracy'])\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if 'loss' in history.history and 'val_loss' in history.history:\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('Model Loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
    "            plt.savefig(tmpfile.name)\n",
    "            mlflow.log_artifact(tmpfile.name, artifact_path=\"evaluation_plots/learning_curves\")\n",
    "        plt.close()\n",
    "        print(\"Learning curves plot logged as artifact.\")\n",
    "\n",
    "    mlflow.set_tag(\"model_type\", \"TensorFlow_DNN\")\n",
    "    print(f\"MLflow Run {run_name} (ID: {run_id}) finished successfully.\")\n",
    "\n",
    "# --- 4. Disable Autologging (Good Practice) ---\n",
    "mlflow.tensorflow.autolog(disable=True)\n",
    "print(\"\\nMLflow autologging for TensorFlow disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7751ab7",
   "metadata": {},
   "source": [
    "# **Phase 8: Model Deployment**\n",
    "\n",
    "## Finding the best model from parameter tunning experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6f2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run ID: 5b426b4e758a426792dfb4b0b1fa1458\n",
      "Best test PR AUC: 0.8309769945531836\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Environment Setup and Start MLflow Run ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials and URI from environment variables\n",
    "MLFLOW_USERNAME = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "MLFLOW_PASSWORD = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "# These lines are crucial for MLflow to authenticate with your server\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_USERNAME\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_PASSWORD\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# 1. Initialize client (connects to MLFLOW_TRACKING_URI env var or default)\n",
    "client = MlflowClient()  \n",
    "\n",
    "# 2. Specify your experiment ID or name\n",
    "experiment_id = client.get_experiment_by_name(\"Adult_Classification_Tuning_XGboost\").experiment_id  # :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 3. Query runs ordered by your metric, descending, limit to top-1\n",
    "best_runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"\",\n",
    "    run_view_type=1,  # ViewType.ACTIVE_ONLY\n",
    "    max_results=1,\n",
    "    order_by=[\"metrics.test_pr_auc DESC\"]  # Replace with your primary metric :contentReference[oaicite:1]{index=1}\n",
    ")\n",
    "best_run = best_runs[0]\n",
    "print(f\"Best run ID: {best_run.info.run_id}\")\n",
    "print(f\"Best test PR AUC: {best_run.data.metrics['test_pr_auc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6566035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model URI: runs:/5b426b4e758a426792dfb4b0b1fa1458/model\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "print(f\"Model URI: {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401193c",
   "metadata": {},
   "source": [
    "### Registering the model in model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a25c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'XGBoost_AdultIncome_BestModel'.\n",
      "2025/05/18 17:45:56 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: XGBoost_AdultIncome_BestModel, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model version: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'XGBoost_AdultIncome_BestModel'.\n"
     ]
    }
   ],
   "source": [
    "model_details = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=\"XGBoost_AdultIncome_BestModel\"  # :contentReference[oaicite:2]{index=2}\n",
    ")\n",
    "print(f\"Registered model version: {model_details.version}\")  # :contentReference[oaicite:5]{index=5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ef44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: aliases=[], creation_timestamp=1747570556188, current_stage='None', description='', last_updated_timestamp=1747570556188, name='XGBoost_AdultIncome_BestModel', run_id='5b426b4e758a426792dfb4b0b1fa1458', run_link='', source='wasbs://artifactroot@tharindumlflow615422a9.blob.core.windows.net/1/5b426b4e758a426792dfb4b0b1fa1458/artifacts/model', status='READY', status_message=None, tags={}, user_id='', version='1'>\n"
     ]
    }
   ],
   "source": [
    "print(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf02f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://artifactroot@tharindumlflow615422a9.blob.core.windows.net/1/5b426b4e758a426792dfb4b0b1fa1458/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "print(model_details.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a58b8b",
   "metadata": {},
   "source": [
    "### Transition to ‚ÄúStaging‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/56hv4rgj2yn_2z_m3dwvcwhc0000gn/T/ipykernel_18435/2952949897.py:9: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model XGBoost_AdultIncome_BestModel v1 is now in Staging.\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "# Replace with your model name and version from registration step\n",
    "model_name = \"XGBoost_AdultIncome_BestModel\"\n",
    "model_version = model_details.version  \n",
    "\n",
    "# Transition to ‚ÄúStaging‚Äù\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=\"Staging\"\n",
    ")\n",
    "print(f\"Model {model_name} v{model_version} is now in Staging.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d0e19",
   "metadata": {},
   "source": [
    "### Transition to ‚ÄúProduction‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f3fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/56hv4rgj2yn_2z_m3dwvcwhc0000gn/T/ipykernel_18435/1642190109.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model XGBoost_AdultIncome_BestModel v1 is now in Production.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "print(f\"Model {model_name} v{model_version} is now in Production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9fcbd",
   "metadata": {},
   "source": [
    "### Load your model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d02e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: models:/XGBoost_AdultIncome_BestModel/Production\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharindu/git/mlflow_iris_example/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00,  9.79it/s]  \n"
     ]
    }
   ],
   "source": [
    "model_name = \"XGBoost_AdultIncome_BestModel\"\n",
    "stage = \"Production\"  # or \"Staging\", or None if you want latest\n",
    "model_uri = f\"models:/{model_name}/{stage or 'latest'}\"\n",
    "print(\"Loading:\", model_uri)\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84e4c2",
   "metadata": {},
   "source": [
    "### Build a sample input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa6a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "workclass",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fnlwgt",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "education",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "education-num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "marital-status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "occupation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "relationship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "race",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sex",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capital-gain",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "capital-loss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hours-per-week",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "native-country",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8f57b8c7-9eab-4523-900a-ac96a6493494",
       "rows": [
        [
         "0",
         "38",
         "Private",
         "215646",
         "HS-grad",
         "9",
         "Divorced",
         "Handlers-cleaners",
         "Not-in-family",
         "White",
         "Male",
         "0",
         "0",
         "40",
         "United-States"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt education  education-num marital-status  \\\n",
       "0   38   Private  215646   HS-grad              9       Divorced   \n",
       "\n",
       "          occupation   relationship   race   sex  capital-gain  capital-loss  \\\n",
       "0  Handlers-cleaners  Not-in-family  White  Male             0             0   \n",
       "\n",
       "   hours-per-week native-country  \n",
       "0              40  United-States  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Re-use your example function, or just hard-code one row:\n",
    "sample = {\n",
    "    'age': 38,\n",
    "    'workclass': 'Private',\n",
    "    'fnlwgt': 215646,\n",
    "    'education': 'HS-grad',\n",
    "    'education-num': 9,\n",
    "    'marital-status': 'Divorced',\n",
    "    'occupation': 'Handlers-cleaners',\n",
    "    'relationship': 'Not-in-family',\n",
    "    'race': 'White',\n",
    "    'sex': 'Male',\n",
    "    'capital-gain': 0,\n",
    "    'capital-loss': 0,\n",
    "    'hours-per-week': 40,\n",
    "    'native-country': 'United-States'\n",
    "}\n",
    "input_df = pd.DataFrame([sample])\n",
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219abbd",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d135b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0  (<=50K=0, >50K=1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted class: {preds[0]}  (<=50K=0, >50K=1)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
